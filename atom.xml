<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[张翼腾的博客]]></title>
  <link href="https://sillybun.github.io/atom.xml" rel="self"/>
  <link href="https://sillybun.github.io/"/>
  <updated>2019-04-03T00:45:21+08:00</updated>
  <id>https://sillybun.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Noise Contrastive Estimation and Negative Sampling]]></title>
    <link href="https://sillybun.github.io/15541232015450.html"/>
    <updated>2019-04-01T20:53:21+08:00</updated>
    <id>https://sillybun.github.io/15541232015450.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">背景</h2>

<p>在NLP领域，使用分数来得到一个概率分布是一种非常常见的形式。我们通常通过取指数然后归一化来得到一个概率分布：</p>

<p>\[<br/>
P(y|x;\theta) = \frac{\mathrm{s}(y,x,\theta)}{\sum\limits_{y\in\mathbf{Y}}s(y,x,\theta)}<br/>
\]</p>

<p>我们称\(Z(x,\theta) = \sum\limits_{y\in\mathbf{Y}}s(y,x,\theta)\)为partition funciton（划分函数）。 在Word2Vec中\(s(y,x,\theta) = \theta_y&#39;v_x\)。标准的学习过程是最大化训练样本的似然函数\[\hat\theta=\arg\max\sum\limits_{(x,y)\in \mathbf{TD}}[\ln\mathrm{s}(y,x,\theta) - \ln\sum\limits_{y\in\mathbf{Y}}s(y,x,\theta)]<br/>
\]，但是计算这个概率（和它的导数）的计算量是非常大的（和词表的大小成正比）。<br/>
为此，NLP科学家们提出了各种方法，一种是基于分类树方法（hierarchical softmax），但是它的缺点在于1. 结果对不同的树比较敏感2.对比较罕见的词效果比较好，但是对常见的单词效果比较差。另一种方法是今天我们要讲的Noise Contrastive Estimation(NCE)和Negative Sampling。</p>

<h2 id="toc_1">Noise Contrastive Estimation</h2>

<p>NCE通过引入负样本把一个多分类问题的参数估计转化成一个区分正负样本的二分类问题。其中负样本是从一个噪声分布中抽取。<br/>
NCE的思想是：<strong>通过区分样本和人造的噪声来训练参数</strong>。<br/>
它的做法是这样的，样本X服从分布\(p_d(\cdot)\)，它是一族分布中的一个, 这族分布中的任何一个可以由一个未归一化的概率密度定义：\(p_0(\cdot, \theta)\)，所以他的归一化的概率密度为：\(p_m(\cdot, \alpha) = \frac{p_0(\cdot,\alpha)}{\int p_0(u, \alpha)\mathrm{d}u}\)。也就是说，存在着\(\alpha^\star\)，使得\(p_d(\cdot) = p_m(\cdot, \alpha^\star)\)，我们可以通过最大似然法来估计\(\alpha^\star\)</p>

<p>于是对数似然函数是：</p>

<p>\[<br/>
\begin{split}<br/>
\ell &amp;= \sum\limits_{x\in \mathrm{TD}} \ln(p_m(x,\theta))\\<br/>
&amp;= \sum\limits_{x\in \mathrm{TD}}[\ln(p_0(x, \theta)) - \ln\int p_0(u,\theta)\mathrm{d}u]<br/>
\end{split}<br/>
\]</p>

<p>我们用常数c来表示它的归一化常数（\(c = \int  p_0(u,\alpha)\mathrm{d}u\)），令\(\theta = \{\alpha, c\}\)，参数c是归一化常数\(Z(\alpha)\)的估计，注意到\(\alpha\)和c之间是要满足归一化条件的(\(c=\int p_0(u, \alpha)\mathrm{d}u\))。</p>

<p>那么\[\ln p_m(u,\theta) = \ln p_0(u,\alpha) - \ln c\]</p>

<p>我们令\(\mathrm{TD} = (x_1, x_2, \dots, x_n)\)表示训练样本，对于每个训练样本\(x_i\)，我们从一个噪声分布中取出K个人造噪声样本\(\{y_i^1, y_i^2,\dots,y_i^K\}\)，\(Y = \{y_i^j|i\in\{1,2,\dots,T\},j\in\{1,2,\dots, K\}\}\)</p>

<p>令\[G_K(u,\theta) = \ln p_m(u,\theta) - \ln [K \cdot p_n(u)] = \ln p_0(u,\alpha) - \ln c -\ln p_n(u) - \ln K\]，\[\begin{split}H_K(u,\theta) &amp;= \sigma (G(u,\theta))\\<br/>
&amp;=\frac{p_m(u,\theta)}{p_m(u,\theta) + K\cdot p_n(u)}<br/>
\end{split}\]</p>

<p>我们对估计参数\(\hat\theta\)是最大化如下目标函数的\(\theta\)：</p>

<p>\[<br/>
\begin{split}<br/>
J_T(\theta) &amp;= \frac{1}{(K+1)T}\sum\limits_{t}[\ln H_K(x_t,\theta) + \sum_j\ln[1-H_K(y_t^j,\theta)]]\\<br/>
&amp; = \frac{1}{(K+1)T}\sum\limits_{t}[\ln \frac{p_m(x_t,\theta)}{p_m(x_t,\theta) + K\cdot p_n(u)} + \sum_j\ln \frac{p_m(y_t^i,\theta)}{p_m(y_t^i,\theta) + K\cdot p_n(u)}]<br/>
\end{split}<br/>
\]</p>

<h3 id="toc_2">NCE和有监督学习之间的关系</h3>

<p>我们这里阐述NCE是如何能够通过一个有监督的二分类问题得到：<strong>我们通过训练区分样本和噪声，能够通过统计模型学到数据的信息</strong>。</p>

<p>我们令\(U = X\cup Y = \{u_1,u_2,\dots,u_{TK}\}\)，对于\(U\)中的任何一个元素\(u_t\)，定一个标签\(\kappa_t\)：若\(u_t\in X\)，\(\kappa_t=1\)；否则\(\kappa_t=0\)。我们通过逻辑斯特回归来得到给定样本它属于哪个标签的后验概率。</p>

<p>\[p(u|\kappa=1,\theta) = p_m(u,\theta)~~ p(u|\kappa=0,\theta)=p_n(u)\]</p>

<p>因为先验概率\(P(\kappa=1) = \frac{1}{K+1}~P(\kappa=0) = \frac{K}{K+1}\)，所以后验概率<br/>
\[P(\kappa=1|u,\theta) = \frac{p_m(u,\theta)}{p_m(u,\theta) + K\cdot p_n(u)}\]<br/>
\[P(\kappa=0|u,\theta) = \frac{K\cdot p_n(u)}{p_m(u,\theta) + K\cdot p_n(u)}\]</p>

<p>因此，我们的对数似然函数为：</p>

<p>\[<br/>
\ell = \sum\limits_{t}[\ln h_K(x_t,\theta) + \sum_j [1-h_K(y_t^j,\theta)]]<br/>
\]<br/>
我们可以看到两个对数似然函数只相差了一个常数。</p>

<h3 id="toc_3">NCE分析</h3>

<p>根据弱大数定理，\(J_{T}\)依概率收敛到\(J\)，\[J = \mathrm{E}[\ln H_K(x,\theta) + K\ln[1-H_K(y,\theta)]]\]</p>

<p>我们定义一个泛函：\[\tilde J(f) = \mathrm{E}[\ln \sigma(f(x) - \ln [K\cdot p_n(x)]) + K\ln[1-\sigma(f(y) - \ln[K\cdot p_n(y)])]]\]</p>

<p>定理1: \(\tilde J\) 在 \(f(\cdot) = \ln p_d(\cdot)\)取得最大值。并且这个最大值点是唯一的如果\(\mathrm{supp}~p_n \subset \mathrm{supp}~p_d\)<br/>
简单起见，我们证明一维的情况。</p>

<p>证明：<br/>
令\(f(\cdot) = \ln g(\cdot)\)</p>

<p>\[<br/>
\begin{split}<br/>
\tilde J(f) &amp;= E[\ln\frac{g(x)}{g(x)+K\cdot p_n(x)} + K \ln\frac{K\cdot p_n(y)}{g(y) + K\cdot p_n(y)}]\\<br/>
&amp;= \int \ln\frac{g(t)}{g(t)+K\cdot p_n(t)}p_d(t) + K \ln\frac{K\cdot p_n(t)}{g(t) + K\cdot p_n(t)}\mathrm{d}t\\<br/>
\end{split}<br/>
\]</p>

<p>我们令\[F(x,y) = \ln\frac{y}{y+K\cdot p_n(x)}p_d(x) + K \ln\frac{K\cdot p_n(x)}{y + K\cdot p_n(x)}\]<br/>
根据欧拉-拉格朗日方程，我们可以得到变分问题取极值的条件为：\[\frac{\partial F}{\partial y} = 0\]</p>

<p>经过化简可以得到，也即：\((p_d(x)-y)p_n(x) = 0\)。这就证明了我们的定理1。<br/>
定理1最重要的一点是它对\(f\)没有任何归一化的要求。并且通过定理1我们可以得到以下几点：</p>

<ol>
<li>我们通过NCE方法无法在取不到负样本的区域作出任何的推断。</li>
<li>我们在优化的过程中是通过控制参数\(\theta\)取到的，我们学到的有两组参数：\(\alpha\)和归一化常数c。</li>
</ol>

<h3 id="toc_4">噪声选择</h3>

<p>噪声分布需要满足下列条件：</p>

<ol>
<li>它是很容易生成的</li>
<li>它的对数密度函数需要是解析的</li>
<li>它取得比较小的MSE \(\mathrm{E}\|\hat\theta -\theta^\star\|^2\)</li>
</ol>

<p>为了满足第三点，噪声函数需要和数据分布尽量的接近，因为如果不这样的话，分类问题会变得很容易，也就是说不需要学到很好的数据结构就能够得到比较好的分类结果，事实上：如果噪声函数就等于数据分布的话，那么我们估计的方差渐进的等于两倍的CR下界。因此，我们一般需要顾及一个比较简单的数据模型，然后根据这个简单的数据模型来得到噪声分布。</p>

<h2 id="toc_5">Word2Vec中的NCE</h2>

<p>Word2Vec中的NCE和原始版本的NCE是有一定的差别的，这里我们给出阐述：</p>

<h3 id="toc_6">Conditional NCE</h3>

<p>我们在Word2Vec中考虑的是如下的参数估计：\[p_m(y|x,\alpha) = \frac{p_0(x,y;\alpha))}{Z(x;\alpha)}\]，其中\(p_0(x,y,\alpha) = \exp(s(x,y,\alpha))\)，s为某种分数。在Word2Vec中，\(s(x,y;\alpha) = \alpha_x^T\alpha_y\)，其中y是一个单词，x是它的上下文(context)中的一个单词，\(\alpha_x\in \mathbb{R}^d\)和\(\alpha_y\in \mathbb{R}^d\)是x和y对应的向量表示。</p>

<p>\(Z(x;\alpha) = \int p_0(x,y;\theta)\mathrm{d}y\)是划分函数，对于固定的\(\alpha\)它是x的函数。</p>

<p>我们对于训练集合\(\mathrm{TD} = \{(x_1,y_1),(x_2,y_2),\dots,(x_T,y_T)\}\)中的每一个样本\((x_i, y_i)\)，根据某种噪声分布\(p_n(\cdot)\)得到K个人造的噪声样本\((x_i,y_i^1),(x_i,y_i^2),\dots,(x_i,y_i^K)\)，和NCE一样，我们把样本和噪声并在一起构成一个新的集合U，并且对于U中的每一个元素，引入一个标签\(\kappa\)，如果来源于原始样本，标签值为1；否则为0。我们令\(G((x,y),\alpha) = \ln p_m(y|x,\alpha) - \ln(K\cdot p_n(y))\),和NCE的推导方式一样，我们可以根据贝叶斯公式得到后验概率：<br/>
\[<br/>
\begin{split}<br/>
P(\kappa &amp;= 1|(x,y),\alpha) = \frac{p_m(y|x,\alpha)}{p_m(y|x,\alpha) + K\cdot p_n(y)}\\<br/>
&amp;= \sigma(G((x,y),\alpha))<br/>
\end{split}<br/>
\]</p>

<p>\[<br/>
\begin{split}<br/>
P(\kappa = 0|(x,y),\alpha) &amp;= \frac{K\cdot p_n(y)}{p_m(y|x,\alpha) + K\cdot p_n(y)}\\<br/>
&amp;=1-\sigma(G((x,y),\alpha))<br/>
\end{split}<br/>
\]</p>

<p>进一步的，模型作出了非常大的一步简化：直接令\(p_m(y|x,\alpha) = p_0(x,y,\alpha) = \exp(s(x,y,\alpha))\)；</p>

<p>事实上\[\ln p_m(y|x,\alpha) = s(x,y,\alpha) - \ln\int\exp(s(x,t,\alpha))\mathrm{d}t\]，令\(c_x = \int\exp(s(x,t,\alpha))\mathrm{d}t\)，就有：\[\ln p_m(y|x,\alpha) = s(x,y,\alpha) - \ln c_x\]（这里需要注意的是\(c_x\)与\(\alpha\)具有约束条件），也就是说，有一个很重要的假设（虽然没有明确说出）:<br/>
\[\forall \alpha,\forall c_{(\cdot)}, \exists \hat\alpha, s.t. s(x,y,\hat\alpha) = s(x,y,\alpha) - \ln c_x, \forall x,y\]</p>

<p>那么对于区分标签的二分类问题，我们优化如下目标函数：</p>

<p>\[<br/>
J(\theta) = \sum_{i=1}^{T}[\ln \sigma((G(x_i, y_i),\alpha)) + \sum_{j=1}^K\ln[1-\sigma(G((x_i,y_i^j),\alpha))]]<br/>
\]</p>

<h3 id="toc_7">Conditional NCE分析</h3>

<p>类似于NCE的定理1的分析，Conditional NCE正确性依赖于如下猜想：</p>

<p>我们定义一个泛函：\[\tilde J(f) = \mathrm{E}[\ln \sigma(f(x) - \ln [K\cdot p_n(x)]) + K\ln[1-\sigma(f(y) - \ln[K\cdot p_n(y)])]]\]</p>

<h2 id="toc_8">参考文献：</h2>

<ol>
<li>Michael Gutmann and Aapo Hyvärinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proc. AISTATS.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2Vec入门]]></title>
    <link href="https://sillybun.github.io/15540423884690.html"/>
    <updated>2019-03-31T22:26:28+08:00</updated>
    <id>https://sillybun.github.io/15540423884690.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Background</h2>

<p>@todo</p>

<h2 id="toc_1">Model Introduction</h2>

<p>在Word2Vec中，Mikolov提出了两个模型 — Continuous Bag-of-Words（CBOW）模型和Skip-gram（SG）模型。两个模型都有三层：输入层，投影层和输出层。对于CBOW模型，输入上下文预测单词；对于SG模型，通过单词预测上下文。目前来看，一般认为SG的效果好于CBOW模型。</p>

<span id="more"></span><!-- more -->

<h3 id="toc_2">CBOW Model</h3>

<p>CBOW包含三层：</p>

<ol>
<li>输入层，包含了context(w)中的2n个单词：\(e(w_{-n}), e(w_{-n+1}), \dots, e(w_n)\)</li>
<li>投影层：投影层是输入层的求和，即：\(x_w = \sum\limits_{i=-n}^{n}e(w_i)\)</li>
<li>输出层：有两种方法：一种是多层的softmax，一种是negative sampling<br/>
优化的函数是：<br/>
\[<br/>
L = \sum\limits_{w\in C}\ln \mathrm{P}(w|\mathrm{context}(w))<br/>
\]</li>
</ol>

<h3 id="toc_3">Hierarchical Softmax</h3>

<p>由于如果对词典中的每个单词使用softmax的计算量非常的大，Word2Vec使用了多层的Softmax方法。我们根据词频建立的<a href="15540295441807.html">Huffman树</a>，这棵树包含了\(|D|\)个叶子和\(|D|-1\)个内点，每个叶子节点对应着词典中的一个单词，出现频率比较高的单词深度会比较的浅，因此到达它的路径会更短。由于到叶子的平均深度为\(\log_2|D|\)，所以对于每一个\((w, \mathrm{context}(w))\)，需要训练的向量的个数的数学期望为：\(\log_2|D|\)。<br/>
到每个叶子节点的路径是唯一的，我们做如下的约定：令p为从根到叶子节点w的路径，令\(n(w, j)\)是p上的第j个节点，\(L(w)\)是p的长度，则\(n(w,1)=\mathrm{root}\)，\(n(w, L(w))=w\)。对于任何的内点n，我们令\(ch(n)\)为n的任何一个选定的子节点，不失一般性的我们可以令它为n的左子节点。令\([\![x]\!]\)为1，如果x为真，否则为-1，那么我们可以根据如下公式定义\(p(w_O|w_I)\)：<br/>
\[<br/>
p(w|w_I) = \prod\limits_{j=1}^{L(w)-1}\sigma([\![n(w, j+1)=ch(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T v_{w_I})<br/>
\]<br/>
我们下面证明：<br/>
引理1：\(\sum\limits_{w\in C} p(w|w_I) = 1\)<br/>
证明：<br/>
令T是C构成的Huffman二叉树，根据Huffman树的性质是我们可以令x和y为T中深度最深的两个兄弟，我们在T中去掉x和y，并且把它们的父亲节点命名为z，得到的新的词表称为\(C’\)(\(C’ = C - \{x,y\} + \{z\}\))，对应的完全二叉树命名为\(T’\)。我们下面证明：<br/>
\[<br/>
\sum\limits_{w\in C} p(w|w_I) = \sum\limits_{w\in C’} p’(w|w_I)\<br/>
\]<br/>
事实上：<br/>
\[<br/>
\begin{split}<br/>
\sum\limits_{w\in C} p(w|w_I) - \sum\limits_{w\in C’} p’(w|w_I) &amp;= p(x|w_I) +p(y|w_I) - p(z|w_I)\\<br/>
&amp;=\prod\limits_{j=1}^{L(x)-1}\sigma([\![n(x, j+1)=ch(n(x,j))]\!]\cdot v_{n(x,j)}&#39;^T v_{w_I})\\<br/>
&amp;~~~~+\prod\limits_{j=1}^{L(y)-1}\sigma([\![n(y, j+1)=ch(n(y,j))]\!]\cdot v_{n(y,j)}&#39;^T v_{w_I})\\<br/>
&amp;~~~~-\prod\limits_{j=1}^{L(z)-1}\sigma([\![n(z, j+1)=ch(n(z,j))]\!]\cdot v_{n(z,j)}&#39;^T v_{w_I})\\<br/>
\end{split}<br/>
\]<br/>
显然有这些关系：<br/>
\[<br/>
\begin{align*}<br/>
L(x)=L(y)=L(z)+1\\<br/>
n(x,j) = n(y,j)=n(z,j), \forall 1\leq j\leq L(z)\\<br/>
\end{align*}<br/>
\]<br/>
因此<br/>
\[<br/>
\begin{split}<br/>
&amp;\sum\limits_{w\in C} p(w|w_I) - \sum\limits_{w\in C’} p’(w|w_I)\\<br/>
&amp;= \prod\limits_{j = 1}^{L(z)-1}\sigma([\![n(z,j+1)=ch(n(z,j))]\!]\cdot v_{n(z,j)}&#39;^Tv_{w_I})\\<br/>
&amp;(\sigma([\![x=ch(n(z,L(z))) ]\!] v_{n(z,L(z))}&#39;^T v_{w_I}) + \sigma([\![y=ch(n(z, L(z)))]\!]v_{n(z,L(z))}&#39;^T v_{w_I}) -1)\\<br/>
&amp;=0<br/>
\end{split}<br/>
\]<br/>
最后一个等式是因为：\(\sigma(x) + \sigma(-x) = 1, \forall x\in\mathbf{R}\)。</p>

<p>这样我们通过不断合并最深的两个兄弟节点，就可以最终把Huffman树合并成只有一个根的树，这样我们就证明了引理1的正确性。</p>

<hr/>

<p>根据引理1，我们通过（1）式定义的是一个概率测度。我们可以通过最最大似然法来进行参数估计。</p>

<h3 id="toc_4">梯度计算</h3>

<p>对于CBOW，投影层之后紧跟着的就是hierarchical softmax层，所以：</p>

<p>\[v_{w_I} = x_w = \sum\limits_{i=-n}^{n}e(w_i)\]</p>

<p>\[\mathrm{P}(w|\mathrm{context}(w)) = \prod\limits_{j=1}^{L(w)-1}\sigma([\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)\]</p>

<p>我们优化的目标为对数似然函数：</p>

<p>\[<br/>
\begin{split}<br/>
L &amp;= \sum\limits_{w\in C}\ln \mathrm{P}(w|\mathrm{context}(w))\\<br/>
&amp;= \sum\limits_{w\in C}\sum\limits_{j=1}^{L(w)-1}\ln{\sigma([\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)}<br/>
\end{split}<br/>
\]</p>

<p>我们令：\[L(w,j) = \ln{\sigma([\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)}\]</p>

<p>在huffman编码中，如果我们令\([\![n(w,j+1)=\mathrm{ch}(n(w,j))]\!]\)为1时编码为0，为-1时编码为1，并且令这个数为\(d(w,j)\)，表示w的huffman编码的第j个数。</p>

<p>\[<br/>
\begin{split}<br/>
\frac{\partial L(w,j)}{\partial v_{n(w,j)}} &amp;= \sigma(-[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]x_w\\<br/>
&amp;= [1 - d(w,j) - \sigma(v_{n(w,j)}&#39;^T x_w)]x_w<br/>
\end{split}<br/>
\]</p>

<p>\[<br/>
\begin{split}<br/>
\frac{\partial L(w,j)}{\partial x_w} &amp;= \sigma(-[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]v_{n(w,j)}&#39;\\<br/>
&amp;= [1 - d(w,j) - \sigma(v_{n(w,j)}&#39;^T x_w)]v_{n(w,j)}<br/>
\end{split}<br/>
\]</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Huffman树]]></title>
    <link href="https://sillybun.github.io/15540295441807.html"/>
    <updated>2019-03-31T18:52:24+08:00</updated>
    <id>https://sillybun.github.io/15540295441807.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">问题</h2>

<p>Huffman树是为了解决这样的问题：给一段文字，能不能给出一个编码方式使得这段文字长度最短。</p>

<h2 id="toc_1">思考</h2>

<p>比如对于<code>aabc</code>，如果我们a编码成00，b编码成01，c编码成10，那么这句话表示为：<code>00000110</code>，一共有8个比特。但是如果a编码成0，b编码成10，c编码成01，那么这句话表示为<code>001011</code>，一共有6个比特。<br/>
看到了么？不同的编码方式下同样的内容对应的长度是不相同的，如何得到一段文字的最佳编码方式呢？Huffman提供了一种算法，他提供了一个贪心的解决了编码问题，并且证明了这种贪心算法得到的答案是最优解。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_2">定义</h2>

<p>我们首先作出以下定义：<br/>
<strong>前缀编码</strong>：如果一个编码方式中没有一个编码是另一个编码的前缀，我们称这种编码方式为前缀编码。<br/>
我们可以看到，如果一种编码方式不是前缀编码，可能会出现二意性的问题，在使用上有很大的不方便。事实上，可以证明，最佳编码方式一定有一种是前缀编码，所以我们把我们的讨论范围限制在前缀编码上。<br/>
我们可以使用一颗二叉树来表示我们的编码方式，每个叶子结点代表一个字符，它的编码由根结点到它的唯一路径表示，如果是左子结点对应着编码0，如果是右子结点对应着编码0。这样的话对于深度为depth（这里认为根结点对应的深度为0）的字符它的编码长度为depth。在这棵二叉树上越浅的位置的字符拥有越少的编码长度。这样来看，我们应该把出现频率高的字符放置在二叉树比较浅的位置，出现频率低的字符放在二叉树比较深的位置。<br/>
显然，我们可以进一步的把我们的讨论范围局限在一颗完全二叉树上面来，这是因为，如果有一个结点只有1个子节点，我们完全可以把这个父亲结点取消掉，直接把子结点连接到父节点的父亲上。子树上的每个叶子结点对应的编码长度都会减少1。<br/>
这样的话，设一种编码方式对应的二叉树为T，对于字符集中的每一个字符c，我们令\(c.freq\)表示c出现的次数，\(d_T(c)\)表示c在树中的深度，这样需要编码这个文件的比特数为：<br/>
\[<br/>
B(T) = \sum\limits_{c\in C}c.freq \cdot d_T(c)<br/>
\]</p>

<h2 id="toc_3">Huffman树的建立</h2>

<p>我们作出以下约定：</p>

<ol>
<li>C表示字符集，里面含有n个字符，其中的每个字符c有一个属性\(c.freq\)</li>
<li>Q表示一个最小堆（优先级队列）<br/>
建立Huffman树的伪代码如下所示：</li>
</ol>

<pre class="line-numbers"><code class="language-python">Huffman(C)
N = |C|
Q = C
for i = 1 to n-1
    allocate a new node z
    z.left = x = Extract-Min(Q)
    z.right = y = Extract-Min(Q)
    z.freq = x.freq + y.freq
    Insert(Q, z)
return Extract-Min(Q)
</code></pre>

<h3 id="toc_4">时间复杂度分析</h3>

<p>建立最小堆的时间复杂度为\(O(n)\)，一共有n-1次循环，每一次循环的时间复杂度为\(O\log n\)，所以Huffman算法的时间复杂度为\(O(n\log n)\)</p>

<h2 id="toc_5">算法正确性的证明</h2>

<p>我们需要证明两个引理。</p>

<hr/>

<p>引理一：对于C中出现次数最低的两个字符x,y，一定存在着一种编码方式，使得x和y的编码长度一样，除了最后一位外其他的位置都相同。<br/>
证明：<br/>
我们只要证明在对应的二叉树上，x和y是兄弟结点就可以了。<br/>
对于一棵最优编码对应的二叉树T，设a和b是它深度最深的叶子兄弟（它们是叶子结点并且它们是兄弟）。假如a和b不是x和y，不妨假设x不是a也不是b。<br/>
我们对换x和a的位置，y和b的位置，得到了一棵新的二叉树\(T’\)<br/>
\[<br/>
\begin{split}B(T) - B(T’) &amp;= x.freq\cdot d_T(x) + a.freq\cdot d_T(a)-x.freq\cdot d_T(a) - a.freq\cdot d_T(x) + \dots \\<br/>
&amp;= (a.freq - x.freq) (d_T(a)-d_T(x)) +  (b.freq - y.freq) (d_T(b)-d_T(y))\\<br/>
&amp;\geq 0<br/>
\end{split}<br/>
\]<br/>
但是由于T是最佳二叉树，所以\(B(T) = B(T’)\)，所以\(T’\)也是完全二叉树，并且在\(T’\)中，x和y是最深的兄弟结点。<br/>
引理二：对于C中出现的次数最低的两个字符x,y，我们在C中去掉x和y，加入一个新的字符z，并且令\(z.freq=x.freq + y.freq\)，建立一个新的字符集\(C’\)。那么对于\(C’\)的最佳二叉树\(T’\)，我们把z对应的叶子结点去掉，放置一个内部结点，并且以x和y作为两个子节点对应的完全二叉树为C对应的一棵最佳二叉树。<br/>
证明：<br/>
我们首先说明\(B(T)\)和\(B(T’)\)之间的关系：\(d_{T’}(z) = d_T(x) + 1\)。<br/>
\[\begin{split}<br/>
B(T) - B(T’) &amp;= x.freq \cdot d_T(x) +y.freq \cdot d_T(y) - z.freq \cdot d_{T’}(z)\\<br/>
&amp;=x.freq \cdot d_T(x) +y.freq \cdot d_T(y) - (x.freq + y.freq) \cdot (d_{T}(x) - 1)\\<br/>
&amp;=x.freq + y.freq<br/>
\end{split}<br/>
\]<br/>
假设T不是C的一棵最佳二叉树，设\(T’’\)是C的一棵最佳二叉树，且根据引理1不妨假设x和y位于最深处并且x和y是兄弟。那么我们把x和y去掉并且把它们的父亲结点命名为z并且令\(z = x.freq + y.freq\)就构成了\(C’\)的一棵二叉树\(T’’’\)</p>

<p>\[\begin{split}<br/>
B(T’’’) &amp;= B(T’’) - x.freq - y.freq\\<br/>
&amp;&lt; B(T) - x.freq - y.freq\\<br/>
&amp;= B(T’) <br/>
\end{split}\]</p>

<p>这与\(T’\)是\(C’\)的最佳二叉树矛盾</p>

<hr/>

<p>根据引理一和引理二我们可以得到Huffman算法的正确性。</p>

]]></content>
  </entry>
  
</feed>
