<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[张翼腾的博客]]></title>
  <link href="https://sillybun.github.io/atom.xml" rel="self"/>
  <link href="https://sillybun.github.io/"/>
  <updated>2019-05-02T23:49:42+08:00</updated>
  <id>https://sillybun.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[The Alias Method: Efficient Sampling with Many Discrete Outcomes]]></title>
    <link href="https://sillybun.github.io/15568111388917.html"/>
    <updated>2019-05-02T23:32:18+08:00</updated>
    <id>https://sillybun.github.io/15568111388917.html</id>
    <content type="html"><![CDATA[
<p>在进行抽样时，我们经常遇到的一个问题是如何能够快速的通过概率分布来重复性的抽取独立离散样本。如果使用累计概率分布表再加上二分类查找能够得到\(O(\ln K)\)的复杂度。但是这是不够好的。事实上，我们可以把问题转化为一个\(1,2,\dots, K\)上的均匀分布和一个两点分布来解决这个问题，预处理的复杂度为\(O(K)\)，抽取一个样本的复杂度将为\(O(1)\)，空间复杂度为\(O(K)\)。</p>

<span id="more"></span><!-- more -->

<p>我们用具体的例子来解释算法：</p>

<p>比如概率分布为：\(\frac{1}{2}, \frac{1}{3}, \frac{1}{6}\)</p>

<ol>
<li>每个概率乘以\(K = 3\)，得到：\(\frac{3}{2}, 1, \frac{1}{2}\)</li>
<li>我们由于\(\frac{3}{2}&gt;1&gt;\frac{1}{2}\)，我们将属于1的\(\frac{1}{2}\)的概率借给3。这样就得到了\(1,1,1\)</li>
<li>但是借过去的\(0.5\)的概率还是属于1号分类的，所以我们就标记3号分类现在有\(0.5\)的概率实际上是1号样本，我们用\(((1,0), (1, 0), (0.5, 0.5:1))\)来表示三号样本中有\(0.5\)的可能性实际上是1号样本。</li>
<li>这样我们在抽取的时候首先在\(1,2,3\)中均匀的抽取一个样本，如果抽取到的是1或者2那么就是抽取到的结果，但是如果抽取到的是3号样本的话</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[pytorch 入门]]></title>
    <link href="https://sillybun.github.io/15556540091713.html"/>
    <updated>2019-04-19T14:06:49+08:00</updated>
    <id>https://sillybun.github.io/15556540091713.html</id>
    <content type="html"><![CDATA[
<span id="more"></span><!-- more -->

<h2 id="toc_0">变量</h2>

<h3 id="toc_1">根据数据产生tensor</h3>

<p>根据数值产生tensor⬇️：</p>

<pre class="line-numbers"><code class="language-python"> torch.tensor(1., requires_grad=True[False])
</code></pre>

<p>根据numpy产生tensor⬇️：</p>

<pre class="line-numbers"><code class="language-python"> torch.tensor(np.array([1., 2., 3.], dtype=np.float32), required_grad=True[False])
</code></pre>

<p>还有一个函数是<code>torch.from_numpy()</code>，它们两者的区别在于：</p>

<ul>
<li> <code>torch.tensor</code>会复制numpy数组数据，所以对新的拷贝进行任何改变并不会改变原来数据的数值；但是<code>torch.from_numpy()</code>共享内存并不会复制数据，对tensor进行的任何改变会对原有数据产生影响。</li>
<li> <code>torch.from_numpy</code>没有<code>required_grad</code>参数，可以这样认为，pytorch并不赞同改变由<code>from_numpy</code>引入的数据。</li>
</ul>

<p>根据list产生tensor⬇️：</p>

<pre class="line-numbers"><code class="language-python"> torch.tensor([1., 2., 3.], required_grad=True[False])
</code></pre>

<h3 id="toc_2">根据分布产生tensor</h3>

<p>0-1均匀分布⬇️：</p>

<pre class="line-numbers"><code class="language-python">torch.rand(3, 3)
# random number of the same size:⬇️
torch.rand_like({some tensor})
</code></pre>

<p>正态分布⬇️：</p>

<pre class="line-numbers"><code class="language-python">torch.randn(10, 3)
torch.randn_lile({some tensor})
</code></pre>

<p>单位矩阵⬇️：</p>

<pre class="line-numbers"><code class="language-python">torch.eye(3)
torch.eye(3,4)
# tensor([[1., 0., 0., 0.],
#         [0., 1., 0., 0.],
#         [0., 0., 1., 0.]])
</code></pre>

<p>全0矩阵⬇️：</p>

<pre class="line-numbers"><code class="language-python">x = torch.zeros(10,3)
torch.zeros_like(x)
</code></pre>

<p>全1矩阵⬇️：</p>

<pre class="line-numbers"><code class="language-python">x = torch.ones(10,3)
torch.ones_like(x)
</code></pre>

<h3 id="toc_3">引入数据</h3>

<p><code>torch.utils.data.DataLoader</code>提供了对数据的引入：</p>

<p>参数有：</p>

<ul>
<li><code>dataset</code>：数据源</li>
<li><code>batch_size</code>：batch_size，默认1</li>
<li><code>shuffle</code>：如果为<code>True</code>：在每一次<code>epoch</code>便会打乱数据。默认<code>False</code></li>
<li><code>drop_last</code>：如果为<code>True</code>：在数据量不能整除<code>batch_size</code>时，会舍弃掉最后一个不完整的batch。默认<code>False</code></li>
</ul>

<p><code>torch.utils.data.random_split</code>提供了对数据的分割：</p>

<p>参数有：</p>

<ul>
<li><code>dataset</code>：数据源</li>
<li><code>lengths</code>：分割出来的每一块的长度</li>
</ul>

<h3 id="toc_4">DataSet</h3>

<pre class="line-numbers"><code class="language-python">class Dataset(object):

    def __getitem__(self, index):
        raise NotImplementedError

    def __len__(self):
        raise NotImplementedError

    def __add__(self, other):
        return ConcatDataset([self, other])
</code></pre>

<p>所以要自己生成dataset需要定义：</p>

<pre class="line-numbers"><code class="language-python">class CustomDataset(torch.utils.data.Dataset):
    def __init__(self):
        # TODO
        # 1. Initialize file paths or a list of file names. 
        pass
    def __getitem__(self, index):
        # TODO
        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).
        # 2. Preprocess the data (e.g. torchvision.Transform).
        # 3. Return a data pair (e.g. image and label).
        pass
    def __len__(self):
        # 有 should change 0 to the total size of your dataset.
        return 0 
</code></pre>

<p>dataset在使用DataLoader导入后会自动的变成tensor。</p>

<pre class="line-numbers"><code class="language-python">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=100, 
                                           shuffle=True)
total_step = len(train_loader)
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # TODO
        if (i+1) % 100 == 0:
            print (&quot;Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}&quot;
                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))
</code></pre>

<h2 id="toc_5">操作</h2>

<h3 id="toc_6">加减法</h3>

<p>加减法可以在相同大小的tensor上进行，是元素的加减⬇️：</p>

<pre class="line-numbers"><code class="language-python">x = torch.tensor([[1., 2., 3.], [2., 2., 2.]])
y = torch.tensor([[0., 1., 2.], [1., 0., 3.]])
x + y #⬇️
# tensor([[1., 3., 5.],
#         [3., 2., 5.]])
</code></pre>

<p>一个维度为\(1\times n\)的行向量可以和一个维度为\(m\times n\)的矩阵相加，其结果为矩阵的每行都加上<br/>
行向量⬇️：</p>

<pre class="line-numbers"><code class="language-python">x = torch.tensor([[1., 2., 3.], [2., 2., 2.]])
z = torch.tensor([1., 2., 3.])
x + z #⬇️
# tensor([[2., 4., 6.],
#         [3., 4., 5.]])
</code></pre>

<p>一个维度为\(m\times 1\)的列向量可以和一个维度为\(m\times n\)的行向量相加，其结果为矩阵的每一列都加上列向量⬇️：</p>

<pre class="line-numbers"><code class="language-python">x = torch.tensor([[1., 2., 3.], [2., 2., 2.]])
z = torch.tensor([[1.], [2.]])
x + z #⬇️
# tensor([[2., 3., 4.],
#         [4., 4., 4.]])
</code></pre>

<p>加减法是自动广播的，两个维度为\(d_1\times d_2\times \dots\times d_{k_1}\)和\(d_1&#39;\times d_2&#39;\times \dots\times d_{k_n}&#39;\)的矩阵（不妨设\(k_1\leq k_2\)）在\((d_{k_1-i} - d_{k_2-i})(d_{k_1-i} - 1)(d_{k_2-i}-1) =0,\forall 0\leq i&lt;k_1\)时可以加减，相当于把每个矩阵的维度都广播为：\(d_1&#39;\times d_2&#39;\times \dots \times d_{k_2-k_1}&#39;\times \max(d_{1}, d_{k_2-k_1+1}&#39;)\times \max(d_{2}, d_{k_2-k_1+2}&#39;)\times \max(d_{k_1}, d_{k_2}&#39;)\)</p>

<h3 id="toc_7">乘除法：</h3>

<h4 id="toc_8">Hadamard Product（逐元素乘法）</h4>

<p>两个维度相同的矩阵可以做乘法和除法，表示对应位置的乘法和除法⬇️：</p>

<pre class="line-numbers"><code class="language-python">x = torch.tensor([[1., 2., 3.], [4., 5., 6.]])
y = torch.tensor([[1., 1., 1.], [2., 2., 2.]])
x * y #⬇️
# tensor([[ 1.,  2.,  3.],
#        [ 8., 10., 12.]])
x / y #⬇️
# tensor([[1.0000, 2.0000, 3.0000],
#        [2.0000, 2.5000, 3.0000]])
</code></pre>

<p>一个维度为\(1\times n\)的行向量可以和一个维度为\(m\times n\)的矩阵做乘法和除法运算，其含义是把矩阵中每一行与行向量做相应的运算⬇️。</p>

<pre class="line-numbers"><code class="language-python">x = torch.tensor([[1., 2., 3.], [4., 5., 6.]])
z = torch.tensor([1., 2., 3.])
x * z # ⬇️
# tensor([[ 1.,  4.,  9.],
#         [ 4., 10., 18.]])
x / z # ⬇️
# tensor([[1.0000, 1.0000, 1.0000],
#         [4.0000, 2.5000, 2.0000]])
z / x # ⬇️
# tensor([[1.0000, 1.0000, 1.0000],
#         [0.2500, 0.4000, 0.5000]])
</code></pre>

<p>一个维度为\(m\times 1\)的列向量可以和一个维度为\(m\times n\)的矩阵做乘法和除法运算，其含义是把矩阵中的每一列与列向量做相应的运算。</p>

<p>乘法和除法是自动广播的，两个维度为\(d_1\times d_2\times \dots\times d_{k_1}\)和\(d_1&#39;\times d_2&#39;\times \dots\times d_{k_n}&#39;\)的矩阵（不妨设\(k_1\leq k_2\)）在\((d_{k_1-i} - d_{k_2-i})(d_{k_1-i} - 1)(d_{k_2-i}-1) =0,\forall 0\leq i&lt;k_1\)时可以加减，相当于把每个矩阵的维度都广播为：\(d_1&#39;\times d_2&#39;\times \dots \times d_{k_2-k_1}&#39;\times \max(d_{1}, d_{k_2-k_1+1}&#39;)\times \max(d_{2}, d_{k_2-k_1+2}&#39;)\times \max(d_{k_1}, d_{k_2}&#39;)\)</p>

<h4 id="toc_9">矩阵乘法</h4>

<p>一个维度为\(m\times n\)的矩阵\(A\)可以和维度为\(n\times p\)的矩阵\(B\)做矩阵乘法，得到维度为\(m\times p\)的矩阵。<strong>注意：pytorch中并不会把一个向量理解成一个矩阵</strong></p>

<pre class="line-numbers"><code class="language-text">x = torch.tensor([[1., 2., 3.], [4., 5., 6.]])
y = torch.tensor([[1., 2.], [3., 4.]])
torch.mm(y, x) #⬇️
# tensor([[ 9., 12., 15.],
#         [19., 26., 33.]])
w = torch.tensor([1., 2.])
torch.mm(w, x) #❌
torch.mm(w.reshape(1, 2), x) #⬇️
# tensor([[ 9., 12., 15.]])
z = torch.tensor([1., 2., 3.])
torch.mm(x, z) #❌
torch.mm(x, z.reshape(3, 1)) #⬇️
# tensor([[14.],
#         [32.]])
</code></pre>

<h4 id="toc_10">广播矩阵乘法</h4>

<p>两个维度为\(n\)的向量\(a, b\)可以做广播矩阵乘法，得到了\(a\cdot b\)。⬇️</p>

<pre class="line-numbers"><code class="language-text">w = torch.tensor([1., 2.])
torch.matmul(w, w)
# tensor(5.)
</code></pre>

<p>一个维度为\(m\)的向量\(v\)可以和一个维度为\(m\times n\)的矩阵\(A\)做广播矩阵乘法，得到了一个维度为\(n\)的向量\(v^\top A\)。⬇️</p>

<pre class="line-numbers"><code class="language-python">x = torch.tensor([[1., 2.], [3., 4.]])
w = torch.tensor([1., 2.])
torch.matmul(w, x)
# tensor([ 7., 10.])
</code></pre>

<p>一个维度为\(m\times n\)的矩阵\(A\)可以和维度为\(n\)的矩阵\(u\)做广播矩阵乘法，得到了一个维度为\(m\)的向量\(A u\)。⬇️</p>

<pre class="line-numbers"><code class="language-python">x = torch.tensor([[1., 2.], [3., 4.]])
w = torch.tensor([1., 2.])
torch.matmul(x, w)
# tensor([ 5., 11.])
</code></pre>

<p>对于两个矩阵之间，广播矩阵乘法等同于矩阵乘法。</p>

<p>如果有某一个矩阵的维度超过2维，那么会把最后两维看作是矩阵，前面的维度都看成是batch被广播。</p>

<p>分为两种情况：只有一个矩阵的维度超过2，对这个矩阵倒数第二个维度前面的所有维度都当成batch；另一种情况是两个矩阵的维度都超过2，那么这两个矩阵的batch必须要匹配：要么一个为1，要么两者都不为1但是一定要相同。比如batch的维度为\(5\times 1\times 3\)与\(1\times 4\times 1\)就是匹配的，两者都被广播成维度为\(5\times 4\times 3\)的batch。</p>

<pre class="line-numbers"><code class="language-python">x = torch.ones([5, 1, 3, 3, 3])
y = torch.ones([1, 4, 1, 3, 3])
torch.matmul(x, y).shape
# torch.Size([5, 4, 3, 3, 3])
</code></pre>

<h3 id="toc_11">改变矩阵的大小</h3>

<pre class="line-numbers"><code class="language-python">reshape(*shape) → Tensor
view(*shape) → Tensor
</code></pre>

<p>两者都能改变矩阵的大小，但是<code>view</code>保证了输入输出共享内存，但是这要求输入的内存是连续保存的，<code>reshape</code>无法保证这一点。</p>

<h3 id="toc_12">转置</h3>

<pre class="line-numbers"><code class="language-python">torch.transpose(input, dim0, dim1) → Tensor
torch.t(input) → Tensor
</code></pre>

<p>第一个函数是交换矩阵中两个维度，输入矩阵的维度为\(d_1 \times d_2 \times \dots \times d_k\)，交换\(i, j\)，那么输出矩阵的维度为：\(d_1 \times d_2 \times \dots\times d_{i-1}\times d_{j}\times d_{i+1}\times  \dots d_{j-1}\times d_i\times d_{j+1}\times \dots \times d_k\)。</p>

<p>第二个函数是<code>torch.transpose(input, 0, 1)</code>的简写，表现出来的就是矩阵的转置。</p>

<h3 id="toc_13">堆叠</h3>

<p><code>torch.stack(seq, dim=0, out=None)</code>：把一串矩阵（需要维度相同）合并成一个新的矩阵在一个新的维度上。</p>

<h3 id="toc_14">合并</h3>

<p><code>torch.cat(tensors, dim=0, out=None) → Tensor</code>：把一串矩阵合并成一个新的矩阵，除却合并的维度外，其他维度要一致。</p>

<pre class="line-numbers"><code class="language-python">&gt;&gt;&gt; x = torch.randn(2, 3)
&gt;&gt;&gt; x
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 0)
tensor([[ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497],
        [ 0.6580, -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497]])
&gt;&gt;&gt; torch.cat((x, x, x), 1)
tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
         -1.0969, -0.4614],
        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
         -0.5790,  0.1497]])
</code></pre>

<h3 id="toc_15">Where</h3>

<p><code>torch.where(condition, x, y) → Tensor</code></p>

<h3 id="toc_16">Max</h3>

<pre class="line-numbers"><code class="language-python">torch.max(input) → Tensor
torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)
</code></pre>

<p>如果只有一个输入，那么就输出矩阵的最大值</p>

<p>如果指定了最大化的维度，那么输出的规则为：输入矩阵\(x\)的维度是\(d_1\times d_2\times \dots \times d_k\)，<code>dim</code>为l，那么输出的维度是：\(d_1\times d_2\times \dots \times d_{l-1}\times d_{l}\times \dots \times d_k\)，并且：\(\mathrm{output}_{i_1,i_2,\dots,i_{l-1},i_{l+1},\dots, i_k} = \max\limits_{0\leq j&lt;d_l}\mathrm{input}_{i_1,i_2,\dots,i_{l-1},j,i_{l+1},\dots, i_k}\)，并且会输出最大值取到的位置，维度是：\(d_1\times d_2\times \dots \times d_{l-1}\times d_{l}\times \dots \times d_k\)，并且：\(\mathrm{location}_{i_1,i_2,\dots,i_{l-1},i_{l+1},\dots, i_k} = \mathop{\arg\max}\limits_{0\leq j&lt;d_l}\mathrm{input}_{i_1,i_2,\dots,i_{l-1},j,i_{l+1},\dots, i_k}\)</p>

<h2 id="toc_17">Layers</h2>

<h3 id="toc_18">Module</h3>

<p>Module是神经网络模块的基本类型，一个Module可以包含子Module，它需要实现初始化方法和<code>forward</code>方法。</p>

<pre class="line-numbers"><code class="language-python">import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
       x = F.relu(self.conv1(x))
       return F.relu(self.conv2(x))
</code></pre>

<p>重要的函数有：</p>

<p><code>apply(fn)</code>：为这个Module和所有的子Module执行<code>fn</code>函数，通常的用处是初始化函数。</p>

<pre class="line-numbers"><code class="language-python">def init_weights(m):
    print(m)
    if type(m) == nn.Linear:
        m.weight.data.fill_(1.0)
        print(m.weight)

net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
net.apply(init_weights)
</code></pre>

<p><code>to()</code>：把这个Module放到cpu或者gpu中：</p>

<pre class="line-numbers"><code class="language-python">device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)
model = NeuralNet(input_size, hidden_size, num_classes).to(device)
</code></pre>

<p><code>zero_grad</code>：把Module所有的grad设为0</p>

<h3 id="toc_19">Linear layer</h3>

<p>\[<br/>
y = xA^\top + b<br/>
\]</p>

<p>对于由<code>nn.Linear(m, n)</code>生成的线形层，它的<code>weight</code>矩阵是一个大小为\(n\times m\)的矩阵，它会把输入的\(m\)个特征经过线性变换变成\(n\)个特征。\(x\)的最后一个维度是特征，其他前面的维度都可以理解为batch。也就是说，输入的大小为：\(b_1\times b_2\times \dots b_k\times m\)，输出的维度为：\(b_1\times b_2\times \dots b_k\times n\)</p>

<pre class="line-numbers"><code class="language-python">linear = nn.Linear(3, 2)
x = torch.randn(10, 3)
pred = linear(x)
</code></pre>

<p>可选参数：</p>

<ol>
<li><code>bias</code>：如果这一项是<code>False</code>，那么不会有<code>bias</code>。</li>
</ol>

<p>注意：</p>

<ol>
<li><code>weight</code>和<code>bias</code>中的每一个元素都从\(U[-\frac{1}{\sqrt{m}}, \frac1{\sqrt{m}}]\)中初始化。</li>
</ol>

<h3 id="toc_20">Convolutional Layer</h3>

<p>\[<br/>
\mathrm{out}(i, j) = \mathrm{bias}^j + \sum\limits_{k=0}^{C_{in}-1}\mathrm{weight}_k^j \star \mathrm{input}(i, k), \forall 0\leq j&lt; C_{out},  0\leq i &lt; n<br/>
\]</p>

<p>输入矩阵的大小为 \(n\times C_{in}\times h_{in}\times w_{in}\)，输出矩阵的大小为\(n\times C_{out}\times h_{out}\times w_{out}\)。</p>

<p>必须的参数有：</p>

<ul>
<li><code>in_channel</code>: \(C_{in}\)</li>
<li><code>out_channel</code>: \(C_{out}\)</li>
<li><code>kernel_size</code>: 卷积核也就是\(\mathrm{weight}\)的大小，提示：如果<code>group=1</code>，那么一共有\(C_{in}C_{out}\)个卷积核。</li>
</ul>

<p>可以选择的参数有：</p>

<ul>
<li><code>stride</code>：默认是1</li>
<li><code>padding</code>：默认是0</li>
<li><code>groups</code>：它必须同时被\(C_{in}, C_{out}\)整除，这时候将输入的channel分为group组，将输出的channel分为同样的组，输出的内容只由它对应组的输入决定。默认值为1</li>
<li><code>bias</code>：默认是<code>True</code></li>
</ul>

<h3 id="toc_21">Max Pooling</h3>

<p><code>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</code>：和卷积原理类似，只是不改变输出的channel，每个channel都独立计算最大值。</p>

<p>注意，它一定不会把padding的0参与最大值的计算。</p>

<h3 id="toc_22">Average Pooling</h3>

<p><code>torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)</code></p>

<p>如果<code>count_include_pad</code>计算平均值不会涉及到padding的0。</p>

<h3 id="toc_23">Batch Normlization</h3>

<p><code>torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</code></p>

<p>\(\mathrm{out} = \frac{x - E[x]}{\sqrt{\mathrm{var}(x)+\mathrm{eps}}}\times \gamma + \beta\)</p>

<p>这里需要注意的是输入的维度是两种情况\(n\times C\times L\)或者是\(n\times L\)，第一种情况下<code>num_features</code>应为\(C\)，归一化是对于\((\cdot, i, \cdot)\)进行；对于第二种情况下<code>num_features</code>应为\(L\)，归一化是对于\((\cdot, i)\)进行。其中保存了<code>num_features</code>个\(\gamma\)和\(\beta\)，并且记录了<code>num_features</code>个均值和方差。</p>

<p>参数解释：</p>

<ul>
<li><code>momentum</code>：统计均值和方差的滑动平均值时的冲量参数，如果为<code>None</code>，那么便记录平均值。 </li>
<li><code>affine</code>：如果为<code>False</code>：那么没有\(\gamma\)和\(\beta\)</li>
<li><code>track_running_states</code>：如果为<code>False</code>，那么不记录均值和方差的均值，每次都现场计算。</li>
</ul>

<p>注意：在训练时会统计均值和方差，但是在评估时，应该调用<code>bn.eval()</code>函数来停止记忆均值和方差。</p>

<p><code>torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</code>：和1d的模块基本一致，但是输入的维度为\(n\times C\times h\times w\)，归一化是针对\((\cdot, i, \cdot, \cdot)\)进行。</p>

<p><code>num_features = C</code></p>

<h4 id="toc_24">LSTM</h4>

<p><code>torch.nn.LSTM(*args, **kwargs)</code></p>

<p>参数：</p>

<ul>
<li><code>input_size</code>：输入的特征维数</li>
<li><code>hidden_size</code>：隐藏层-输出的特征维数</li>
<li><code>num_layers</code>：堆叠的层数</li>
<li><code>bias</code>：默认<code>False</code></li>
<li><code>batch_first</code>：如果为真，输入的维度为：\(n\times L\times p\)，其中\(L\)为输入长度，\(p\)为特征数，否则，输入的维度为：\(L\times n\times p\)。<strong>影响输出维度！！</strong></li>
<li><code>drop_out</code>：如果为非零，在每一层上面加一层dropout层，概率为参数，默认值：0</li>
<li><code>bidirectional</code>：若为<code>True</code>，是双向的LSTM，默认是<code>False</code></li>
</ul>

<p>输入：</p>

<ul>
<li><code>input</code>：维度\(L\times n\times p\)，如果<code>batch_first==False</code>；否则：\(n\times L\times p\)</li>
<li><code>h_0</code>, <code>c_0</code>：维度\(\mathrm{num\_layers}*\mathrm{num\_directions}\times n\times p_\mathrm{hidden}\)</li>
</ul>

<p>输出：</p>

<ul>
<li><code>output</code>：维度\(L\times n\times \mathrm{num\_directions}*p_\mathrm{hidden}\)，可以使用<code>output.vew(L, n, num_directions, hidden_size)</code>来分离两个方向。如果<code>batch_first</code>，维度\(n\times L\times \mathrm{num\_directions}*p_\mathrm{hidden}\)</li>
<li><code>h_n</code>, <code>c_n</code>：维度\(\mathrm{num\_layers}*\mathrm{num\_directions}\times n\times p_\mathrm{hidden}\)，可以使用<code>h_n.view(num_layers, num_directions, n, hidden_size)</code></li>
</ul>

<h2 id="toc_25">Loss</h2>

<h3 id="toc_26">MSELoss</h3>

<p><code>nn.MSELoss(x, y)</code>它的输入两个tensor，并且计算两个tensor之间的Mean Square Error，注意到这是一个可广播的运算，并且是从最后一个维度往前广播。</p>

<p>它计算了：\(\frac{\sum(x_i - y_i)^2}{n}\)。</p>

<p>可选参数：</p>

<ul>
<li><code>reduction=</code>：&#39;mean&#39;(Default)计算平均值，&#39;sum&#39;计算和，&#39;none&#39;等同于<code>(x-y) * (x-y)</code></li>
</ul>

<h3 id="toc_27">CrossEntropyLoss</h3>

<p><code>torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code></p>

<p>它的输入是两个矩阵，第一个矩阵\(x\)的维度需要是\(n\times c\)，其中\(n\)是batch_size, \(c\)是类别的个数，第二个矩阵\(y\)的维度是\(n\)，表示的是各个数据对应的标签。</p>

<p>对于每个batch，loss值为：</p>

<p>\[<br/>
\ell = -\ln\frac{\exp(x_y)}{1^\top \exp(x)}<br/>
\]</p>

<pre class="line-numbers"><code class="language-python">x = torch.randn(10, 3)
y = torch.randint(0, 4, (10,))
torch.nn.CrossEntropyLoss()(x, y)
# tensor(1.9090, grad_fn=&lt;NllLossBackward&gt;)
one_vector = torch.ones(3, 1)
exp_x = torch.exp(x)
x_distribution = exp_x / torch.mm(exp_x, one_vector)
ell = 0
for k in range(10):
    ell = ell - torch.log(x_distribution[k, y[k]])
print(ell / 10)
# tensor(1.9090, grad_fn=&lt;DivBackward0&gt;)
</code></pre>

<h2 id="toc_28">训练</h2>

<h3 id="toc_29">Backward</h3>

<p>对于一个值\(a\)是可以回传梯度的。这时候，对于<code>required_grad=True</code>的tensor就会有<code>grad</code>这个属性，保存了\((\frac{\partial a}{\partial W})_{1,1}\)</p>

<p>需要注意的有亮点：</p>

<p>1: 对于一个线性的计算图\(\ell = f_1f_2\dots f_n(A)\)，一旦我们得到了\(\frac{\partial \ell}{\partial A}\)，并且\(A = f(B)\)，那么根据链式法则：</p>

<p>\[<br/>
\frac{\partial \ell}{\partial B} = \frac{\partial \ell}{\partial A}\frac{\partial A}{\partial B}<br/>
\]</p>

<p>这样意味着在计算过程中是需要保存为了计算\(\frac{\partial A}{\partial B}\)相关的数据的，但是pytorch为了节省空间，在<code>backward</code>中会把这种中间变量删除，所以第二次运行<code>backward</code>时会报错。为了使得不删除这种中间数据，需要在<code>backward</code>时加入<code>retain_graph=True</code>参数。</p>

<p>2: 如果调用了两个<code>backward</code>函数并且它们涉及到着公共的需要求梯度的tensor，梯度之间是相加的关系。</p>

<h3 id="toc_30">优化</h3>

<h4 id="toc_31">优化方法</h4>

<p><code>optimizer = torch.optim.SGD()</code></p>

<p>参数有：</p>

<ul>
<li><code>params</code>: 需要优化的参数</li>
<li><code>lr</code>: 学习率</li>
<li><code>momentum</code>: 动量参数</li>
<li><code>weight_decay</code>: L2正则项</li>
</ul>

<p><code>optimizer.step()</code>：进行一步优化（并不包含<code>backward</code>操作）<br/>
<code>optimizer.zero_grad()</code>：将模型中的参数的梯度设为0</p>

<p>所以一般的流程是：</p>

<pre class="line-numbers"><code class="language-python">for t in range(time_steps):
    loss = criterion(pred, target)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
</code></pre>

<hr/>

<p><code>optimizer = torch.optim.Adam(params, lr=1e-3)</code></p>

<h4 id="toc_32">更新学习率</h4>

<pre class="line-numbers"><code class="language-python">def update_lr(optimizer, lr):    
    for param_group in optimizer.param_groups:
        param_group[&#39;lr&#39;] = lr
        
update_lr(optimizer, new_lr)
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[信息论]]></title>
    <link href="https://sillybun.github.io/15543921008119.html"/>
    <updated>2019-04-04T23:35:00+08:00</updated>
    <id>https://sillybun.github.io/15543921008119.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">熵</h2>

<p>假设离散随机变量X服从概率分布P。我们用它熵表示一个随机变量的不确定程度，用\(\mathbb{H} (X)\)或者是\(\mathbb{H}(p)\)表示：\[\mathbb{H}(X)\mathop{=}\limits^\Delta -\sum\limits_{k=1}^K p(X=k)\log p(X=k) = \mathop{E}\limits_{x\sim p(\cdot)}[\log(p(x))]\]</p>

<span id="more"></span><!-- more -->

<p>定理1: 在所有取值为\(\{1,2,\dots, K\}\)的离散随机变量，均匀分布取到最大熵。</p>

<p>这个定理的证明我会把它放到后面。根据这个定理我们可以得到Laplace&#39;s principle of insufficient reason. 当不能提供分布的任何信息的时候，我们偏好均匀分布，因为它保留了最大的不确定性。</p>

<h2 id="toc_1">KL divergence</h2>

<p>KL divergence（又称为相对熵），衡量了两种概率分布之间的不相似性：\[\mathbb{KL}(p\|q) = \sum_{k=1}^{K}p_k\log\frac{p_k}{q_k}=\mathop{\mathbb{E}}\limits_{x\sim  p(\cdot)}\log\frac{p(x)}{q(x)}\]</p>

<p>我们可以把KL divergence重新写为：\[\mathbb{KL}(p\|q) = \mathop{\mathbb{E}}\limits_{x\sim  p(\cdot)}\log p(x) - \mathop{\mathbb{E}}\limits_{x\sim p(\cdot)}\log {q(x)} = \mathbb{H}(p,q) - \mathbb{H}(p)\]</p>

<p>其中\[\mathbb{H}(p,q) = - \mathop{\mathbb{E}}\limits_{x\sim p(\cdot)}\log {q(x)}\]被称为交叉熵。</p>

<p>定理2：\(\mathbb{KL}(p,q) \geq 0\). 当且仅当\(p=q\)时相对熵等于0。</p>

<p>证明：<br/>
\[\begin{split}\mathbb{KL}(p,q) &amp;= \sum_{k=1}^{K}p_k\log\frac{p_k}{q_k}\\<br/>
&amp;= - \sum_{k=1}^{K}p_k\log\frac{q_k}{p_k}\\<br/>
&amp;\geq - \log \sum_{k=1}^{K}p_k\frac{q_k}{p_k}\\<br/>
&amp;= 0\\<br/>
\end{split}<br/>
\]</p>

<p>最后一个大于等于号是由于\(\log(\cdot)\)函数是concave函数。并且可以看到等号成立当且仅当存在c，使得\(p(x) = cq(x)\forall x\)，但是由于\(p(x)\)和\(q(x)\)都是自归一化的，所以\(c=1\)。</p>

<p>因此，\(\mathbb{KL}(p,q) =0\)当且仅当\(p=q\)。</p>

<p>我们使用这个定理来证明定理1。令\(p(x)\)是\(\{1,2,\dots,K\}\)上的概率函数，\(u(x)\)是其上的均匀分布。根据定理2,\[\begin{split}\mathbb{KL}(p\|u) &amp;=  \mathop{\mathbb{E}}\limits_{x\sim p(\cdot)} \log p(x) - \mathop{\mathbb{E}}\limits_{x\sim p(\cdot)} \log u(x)\\<br/>
&amp;= \mathop{\mathbb{E}}\limits_{x\sim p(\cdot)} \log p(x) + \mathop{\mathbb{E}}\limits_{x\sim u(\cdot)} \log u(x) \\&amp;= \mathbb{H}(u) - \mathbb{H}(p)\geq 0\end{split}\]</p>

<h2 id="toc_2">共同信息MI</h2>

<p>我们考虑两个随机变量X和Y我们来衡量两个随机变量之间的独立情况，我们可以用相关系数来衡量，但是相关性并不能完全的衡量独立性。为此我们引入共同信息（Mutual infromation or MI）。是这样定义的：\[\mathbb{I}(X;Y) \mathop{=}\limits^\Delta \mathbb{KL}(p(X,Y)\|P(X)p(Y)) = \sum\limits_x\sum\limits_y p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\]</p>

<p>因此\(\mathbb{I}(X;Y)\geq 0\)。当且仅当X和Y独立时为0。</p>

<p>\[\begin{split}<br/>
\mathbb{I}(X;Y) &amp;= \sum\limits_x\sum\limits_y p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\\<br/>
&amp;= \sum\limits_x\sum\limits_y p(x,y)[\log p(x|y) -  \log p(x)]\\<br/>
&amp;= \sum\limits_{y}p(y)\sum\limits_{x}p(x|y)\log p(x|y) - \sum_{x}p(x)\log p(x)\\<br/>
&amp;= - \sum\limits_{y}p(y)H(Y|X=x) + H(X)\\<br/>
&amp;= H(X) - H(X|Y)\\<br/>
\end{split}\]</p>

<p>逐点互信息\[\mathrm{PMI}(X,Y) \mathop{=}\limits^{\Delta} \log\frac{p(x,y)}{p(x)p(y)} = \log \frac{p(x|y)}{p(x)} = \log \frac{p(y|x)}{p(y)}\]，他刻画了当我们知道y后将\(p(x)\)更新为\(p(x|y)\)所能得到的信息量。</p>

<h2 id="toc_3">Cross entropy loss 与MLE</h2>

<p>在机器学习中一个常见的损失函数是Cross Entopy Loss。这里我们讨论一个多分类的问题，我们要把样本分到编号对应为\(1,2,\dots, n\)的类里面，我们的真实的编号是\(l_{true}\)，我们模型预测的属于某个编号的概率是\(P(l=k) = y_k\)，那么Cross Entropy Loss是：\[\mathrm{loss}_{CE}= -\log y_{l_{true}}\]</p>

<p>我们这里有两种解释，一种是基于Cross Entropy：真实的概率分布是\(P_{true}(l=k) = \delta(k, l_{true})\)，估计的概率分布为\(P_{pred}=y_k\)。那么我们用\(H(P_{true}, P_{pred}) = \mathbb{KL}(P_{true}\|P_{pred})\)，来刻画两者之间的差异程度。根据Cross Entropy的性质\(\mathrm{loss}_{\mathrm{CE}} \geq 0\)，并且当且仅当两者一致时为0。</p>

<p>另一种解释是基于MLE，似然函数就是是：\(y_{l_{true}}\)，所以对应的负对数释然函数就是：\[\ell = -\log y_{l_{true}}\]</p>

<p>因此在标签唯一的情况下，Cross Entropy Loss和MLE是一致的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Noise Contrastive Estimation and Negative Sampling]]></title>
    <link href="https://sillybun.github.io/15541232015450.html"/>
    <updated>2019-04-01T20:53:21+08:00</updated>
    <id>https://sillybun.github.io/15541232015450.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">背景</h2>

<p>在NLP领域，使用分数来得到一个概率分布是一种非常常见的形式。我们通常通过取指数然后归一化来得到一个概率分布：</p>

<p>\[<br/>
\begin{equation}<br/>
P_d(y|x;\alpha) = \frac{\exp(\mathrm{s}(y,x,\alpha))}{\sum\limits_{y\in\mathbf{Y}}\exp(s(y,x,\alpha))}<br/>
\end{equation}<br/>
\]</p>

<p>我们称\(Z(x,\alpha) = \sum\limits_{y\in\mathbf{Y}}\exp(s(y,x,\alpha))\)为partition funciton（划分函数）。 在Word2Vec中\(s(y,x,\alpha) = \alpha_y&#39;v_x\)。标准的学习过程是最大化训练样本的似然函数\[\hat\alpha=\arg\max\sum\limits_{(x,y)\in \mathbf{TD}}[\mathrm{s}(y,x,\alpha) - \ln\sum\limits_{y\in\mathbf{Y}}\exp(s(y,x,\alpha))]<br/>
\]，但是计算这个概率（和它的导数）的计算量是非常大的（和词表的大小成正比）。<br/>
为此，NLP科学家们提出了各种方法，一种是基于分类树方法（hierarchical softmax），但是它的缺点在于1. 结果对不同的树比较敏感2.对比较罕见的词效果比较好，但是对常见的单词效果比较差。另一种方法是今天我们要讲的Noise Contrastive Estimation(NCE)和Negative Sampling。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_1">Noise Contrastive Estimation</h2>

<p>NCE通过引入负样本把一个多分类问题的参数估计转化成一个区分正负样本的二分类问题。其中负样本是从一个噪声分布中抽取。<br/>
NCE的思想是：<strong>通过区分样本和人造的噪声来训练参数</strong>。<br/>
它的做法是这样的，样本X服从分布\(p_d(\cdot)\)，它是一族分布中的一个, 这族分布中的任何一个可以由一个未归一化的概率密度定义：\(p_0(\cdot, \alpha)\)，所以他的归一化的概率密度为：\(p_m(\cdot, \alpha) = \frac{p_0(\cdot,\alpha)}{\int p_0(u, \alpha)\mathrm{d}u}\)。也就是说，存在着\(\alpha^*\)，使得\(p_d(\cdot) = p_m(\cdot, \alpha^*)\)，我们可以通过最大似然法来估计\(\alpha^*\)</p>

<p>于是对数似然函数是：</p>

<p>\[<br/>
\begin{split}<br/>
\ell &amp;= \sum\limits_{x\in \mathrm{TD}} \ln(p_m(x,\alpha))\\<br/>
&amp;= \sum\limits_{x\in \mathrm{TD}}[\ln(p_0(x, \alpha)) - \ln\int p_0(u,\alpha)\mathrm{d}u]<br/>
\end{split}<br/>
\]</p>

<p>我们用常数c来表示它的归一化常数（\(c = \int  p_0(u,\alpha)\mathrm{d}u\)），令\(\theta = \{\alpha, c\}\)，参数c是归一化常数\(Z(\alpha)\)的估计，注意到\(\alpha\)和c之间是要满足归一化条件的(\(c=\int p_0(u, \alpha)\mathrm{d}u\))。</p>

<p>那么\[\ln p_m(u,\theta) = \ln p_0(u,\alpha) - \ln c\]</p>

<p>我们令\(\mathrm{TD} = (x_1, x_2, \dots, x_n)\)表示训练样本，对于每个训练样本\(x_i\)，我们从一个噪声分布中取出K个人造噪声样本\(\{y_i^1, y_i^2,\dots,y_i^K\}\)，\(Y = \{y_i^j|i\in\{1,2,\dots,T\},j\in\{1,2,\dots, K\}\}\)</p>

<p>令\[G_K(u,\theta) = \ln p_m(u,\theta) - \ln [K \cdot p_n(u)] = \ln p_0(u,\alpha) - \ln c -\ln p_n(u) - \ln K\]，\[\begin{split}H_K(u,\theta) &amp;= \sigma (G(u,\theta))\\<br/>
&amp;=\frac{p_m(u,\theta)}{p_m(u,\theta) + K\cdot p_n(u)}<br/>
\end{split}\]</p>

<p>我们对估计参数\(\hat\theta\)是最大化如下目标函数的\(\theta\)：</p>

<p>\[<br/>
\begin{split}<br/>
J_T(\theta) &amp;= \frac{1}{(K+1)T}\sum\limits_{t}[\ln H_K(x_t,\theta) + \sum_j\ln[1-H_K(y_t^j,\theta)]]\\<br/>
&amp; = \frac{1}{(K+1)T}\sum\limits_{t}[\ln \frac{p_m(x_t,\theta)}{p_m(x_t,\theta) + K\cdot p_n(x_t)} + \sum_j\ln \frac{p_m(y_t^i,\theta)}{p_m(y_t^i,\theta) + K\cdot p_n(y_t^i)}]<br/>
\end{split}<br/>
\]</p>

<h3 id="toc_2">NCE和有监督学习之间的关系</h3>

<p>我们这里阐述NCE是如何能够通过一个有监督的二分类问题得到：<strong>我们通过训练区分样本和噪声，能够通过统计模型学到数据的信息</strong>。</p>

<p>我们令\(U = X\cup Y = \{u_1,u_2,\dots,u_{TK}\}\)，对于\(U\)中的任何一个元素\(u_t\)，定一个标签\(\kappa_t\)：若\(u_t\in X\)，\(\kappa_t=1\)；否则\(\kappa_t=0\)。我们通过逻辑斯特回归来得到给定样本它属于哪个标签的后验概率。</p>

<p>\[p(u|\kappa=1,\theta) = p_m(u,\theta)~~ p(u|\kappa=0,\theta)=p_n(u)\]</p>

<p>因为先验概率\(P(\kappa=1) = \frac{1}{K+1}~P(\kappa=0) = \frac{K}{K+1}\)，所以后验概率<br/>
\[P(\kappa=1|u,\theta) = \frac{p_m(u,\theta)}{p_m(u,\theta) + K\cdot p_n(u)}\]<br/>
\[P(\kappa=0|u,\theta) = \frac{K\cdot p_n(u)}{p_m(u,\theta) + K\cdot p_n(u)}\]</p>

<p>因此，我们的对数似然函数为：</p>

<p>\[<br/>
\ell = \sum\limits_{t}[\ln h_K(x_t,\theta) + \sum_j [1-h_K(y_t^j,\theta)]]<br/>
\]<br/>
我们可以看到两个对数似然函数只相差了一个常数。</p>

<h3 id="toc_3">NCE分析</h3>

<p>根据弱大数定理，\(J_{T}\)依概率收敛到\(J\)，\[J = \mathrm{E}[\ln H_K(x,\theta) + K\ln[1-H_K(y,\theta)]]\]</p>

<p>我们定义一个泛函：\[\tilde J(f) = \mathrm{E}[\ln \sigma(f(x) - \ln [K\cdot p_n(x)]) + K\ln[1-\sigma(f(y) - \ln[K\cdot p_n(y)])]]\]</p>

<p>定理1: \(\tilde J\) 在 \(f(\cdot) = \ln p_d(\cdot)\)取得最大值。并且这个最大值点是唯一的如果\(\mathrm{supp}~p_n \subset \mathrm{supp}~p_d\)<br/>
简单起见，我们证明一维的情况。</p>

<p>证明：<br/>
令\(f(\cdot) = \ln g(\cdot)\)</p>

<p>\[<br/>
\begin{split}<br/>
\tilde J(f) &amp;= E[\ln\frac{g(x)}{g(x)+K\cdot p_n(x)} + K \ln\frac{K\cdot p_n(y)}{g(y) + K\cdot p_n(y)}]\\<br/>
&amp;= \int \ln\frac{g(t)}{g(t)+K\cdot p_n(t)}p_d(t) + K \ln\frac{K\cdot p_n(t)}{g(t) + K\cdot p_n(t)}p_n(t)\mathrm{d}t\\<br/>
\end{split}<br/>
\]</p>

<p>我们令\[F(x,y) = \ln\frac{y}{y+K\cdot p_n(x)}p_d(x) + K\cdot p_n(x) \ln\frac{K\cdot p_n(x)}{y + K\cdot p_n(x)}\]<br/>
根据欧拉-拉格朗日方程，我们可以得到变分问题取极值的条件为：\[\frac{\partial F}{\partial y} = 0\]</p>

<p>经过化简可以得到，也即：\((p_d(x)-y)p_n(x) = 0\)。这就证明了我们的定理1。<br/>
定理1最重要的一点是它对\(f\)没有任何归一化的要求。并且通过定理1我们可以得到以下几点：</p>

<ol>
<li>我们通过NCE方法无法在取不到负样本的区域作出任何的推断。</li>
<li>我们在优化的过程中是通过控制参数\(\theta\)取到的，我们学到的有两组参数：\(\alpha\)和归一化常数c。</li>
</ol>

<h3 id="toc_4">噪声选择</h3>

<p>噪声分布需要满足下列条件：</p>

<ol>
<li>它是很容易生成的</li>
<li>它的对数密度函数需要是解析的</li>
<li>它取得比较小的MSE \(\mathrm{E}\|\hat\theta -\theta^*\|^2\)</li>
</ol>

<p>为了满足第三点，噪声函数需要和数据分布尽量的接近，因为如果不这样的话，分类问题会变得很容易，也就是说不需要学到很好的数据结构就能够得到比较好的分类结果，事实上：如果噪声函数就等于数据分布的话，那么我们估计的方差渐进的等于两倍的CR下界。因此，我们一般需要顾及一个比较简单的数据模型，然后根据这个简单的数据模型来得到噪声分布。</p>

<h2 id="toc_5">Word2Vec中的NCE</h2>

<p>Word2Vec中的NCE和原始版本的NCE是有一定的差别的，这里我们给出阐述：</p>

<h3 id="toc_6">Conditional NCE</h3>

<p>我们在Word2Vec中考虑的是如下的参数估计：\[p_m(y|x,\alpha) = \frac{p_0(x,y;\alpha))}{Z(x;\alpha)}\]，其中\(p_0(x,y,\alpha) = \exp(s(x,y,\alpha))\)，s为某种分数。在Word2Vec中，\(s(x,y;\alpha) = \alpha_x^T\alpha_y\)，其中y是一个单词，x是它的上下文(context)中的一个单词，\(\alpha_x\in \mathbb{R}^d\)和\(\alpha_y\in \mathbb{R}^d\)是x和y对应的向量表示。</p>

<p>\(Z(x;\alpha) = \int p_0(x,y;\alpha)\mathrm{d}y\)是划分函数，对于固定的\(\alpha\)它是x的函数。</p>

<p>我们对于训练集合\(\mathrm{TD} = \{(x_1,y_1),(x_2,y_2),\dots,(x_T,y_T)\}\)中的每一个样本\((x_i, y_i)\)，根据某种噪声分布\(p_n(\cdot)\)得到K个人造的噪声样本\((x_i,y_i^1),(x_i,y_i^2),\dots,(x_i,y_i^K)\)，和NCE一样，我们把样本和噪声并在一起构成一个新的集合U，并且对于U中的每一个元素，引入一个标签\(\kappa\)，如果来源于原始样本，标签值为1；否则为0。我们令\(G((x,y),\alpha) = \ln p_m(y|x,\alpha) - \ln(K\cdot p_n(y))\),和NCE的推导方式一样，我们可以根据贝叶斯公式得到后验概率：<br/>
\[<br/>
\begin{split}<br/>
P(\kappa &amp;= 1|(x,y),\alpha) = \frac{p_m(y|x,\alpha)}{p_m(y|x,\alpha) + K\cdot p_n(y)}\\<br/>
&amp;= \sigma(G((x,y),\alpha))<br/>
\end{split}<br/>
\]</p>

<p>\[<br/>
\begin{split}<br/>
P(\kappa = 0|(x,y),\alpha) &amp;= \frac{K\cdot p_n(y)}{p_m(y|x,\alpha) + K\cdot p_n(y)}\\<br/>
&amp;=1-\sigma(G((x,y),\alpha))<br/>
\end{split}<br/>
\]</p>

<p>进一步的，模型作出了非常大的一步简化：直接令\(p_m(y|x,\alpha) = p_0(x,y,\alpha) = \exp(s(x,y,\alpha))\)；</p>

<p>事实上\[\ln p_m(y|x,\alpha) = s(x,y,\alpha) - \ln\int\exp(s(x,t,\alpha))\mathrm{d}t\]，令\(c_x = \int\exp(s(x,t,\alpha))\mathrm{d}t\)，就有：\[\ln p_m(y|x,\alpha) = s(x,y,\alpha) - \ln c_x\]（这里需要注意的是\(c_x\)与\(\alpha\)具有约束条件），也就是说，有一个很重要的假设1（虽然没有明确说出）:<br/>
\[\forall \alpha, \exists \hat\alpha, s.t. s(x,y,\hat\alpha) = s(x,y,\alpha) - \ln c_x, \forall x,y\]，这蕴含着\(\int \exp(s(x,y,\hat\alpha))\mathrm{d}y = 1, \forall x\)</p>

<p>有一个更强的假设：\[\forall \alpha,\forall h_{(\cdot)}, \exists \hat\alpha, s.t. s(x,y,\hat\alpha) = s(x,y,\alpha) - \ln h_x, \forall x,y\]</p>

<p>那么对于区分标签的二分类问题，我们优化如下目标函数：</p>

<p>\[<br/>
J(\alpha) = \sum_{i=1}^{T}[\ln \sigma((G((x_i, y_i),\alpha)) + \sum_{j=1}^K\ln[1-\sigma(G((x_i,y_i^j),\alpha))]]<br/>
\]</p>

<h3 id="toc_7">Conditional NCE分析</h3>

<p>类似于NCE的定理1的分析，Conditional NCE正确性依赖于如下定理：</p>

<p>定理2：我们定义一个泛函：\[\tilde{J} (f(\cdot, \cdot)) = \mathop{\mathrm{E}}\limits_{(x,y)\sim p_d, n\sim p_n}[\ln \sigma(f(x,y) - \ln [K\cdot p_n(y)]) + K\ln[1-\sigma(f(x,n) - \ln[K\cdot p_n(n)])]]\]，这个泛函在\(f(x,y) = \ln p_d(y|x)\)处取得最大值。</p>

<p>证明：</p>

<p>\[\tilde{J} (f(\cdot, \cdot)) = \mathop{\mathrm{E}}\limits_{x\sim {p_d}_x}[\mathop{\mathrm{E}}\limits_{y\sim p_d(\cdot|x), n\sim p_n}[\ln \sigma(f(x,y) - \ln [K\cdot p_n(y)]) + K\ln[1-\sigma(f(x,n) - \ln[K\cdot p_n(n)])]]]\]<br/>
对于固定的x，根据定理1，\[\ln p_d(\cdot|x) = \mathop{\arg\max}\limits_{f(\cdot)}\mathop{\mathrm{E}}\limits_{y\sim p_d(\cdot|x), n\sim p_n}[\ln \sigma(f(y) - \ln [K\cdot p_n(y)]) + K\ln[1-\sigma(f(n) - \ln[K\cdot p_n(n)])]]\] ，进一步的我们就证明了定理2。</p>

<p>因此如果我们的评分函数满足假设1，那么得到的估计参数\(\hat\alpha\)自然的会满足归一化条件：\[\int \exp(s(x,t,\hat\alpha))\mathrm{d}t = 1\]</p>

<h2 id="toc_8">Two estimation algorithm for NCE</h2>

<h3 id="toc_9">自归一化假设</h3>

<p>我们这里给出两个假设：</p>

<p>假设2: 存在着\(\alpha^*\)使得，\(P_d(y|x) = \frac{\exp(s(x,y;\alpha^*))}{Z(x,\alpha^*)}\)，其中\(Z(x,\alpha) = \int\limits_{y}\exp((x,y;\alpha^*))\mathrm{d}y\)</p>

<p>假设3: 存在着\(\alpha^*\)和\(\gamma^*\in \mathbb{R}\)，使得：\[P_d(y|x)=\exp(s(x,y,\alpha^*)-\gamma^*)\]，也就是说\(\gamma^* = \ln Z(x;\alpha^*)\)是与x无关的量。</p>

<p>可以看到，<strong>假设3是严格强于假设2的</strong>。我们称符合假设3的条件分布函数是<strong>自归一化</strong>的。但是着包含着\(|\mathbf{X}|\)个限制条件，但是有\(d+1\)个自由变量，在\(|\mathbf{X}| \gg d+1\)时，假设3是几乎不可能实现的。这是我猜测为什么Skip-Gram方法是好于CBOW方法的一个地方：Skip-Gram是自归一化的，而CBOW不是@todo。</p>

<p>给出满足假设2的分数函数\(s(x,y,\alpha)\)，我们可以重新定义一个满足假设3的分数函数：\[s&#39;(x,y,\alpha,{c_x:x\in\mathbf{X}}) = s(x,y,\alpha) - c_x\]，也就是说我们需要引入一个新的参数\(c_x\)对于每一个可能的历史x，这也是(Minch and Tech, 2012)年的工作中采用的方法，他们使用了CBOW方法，对于每一个context学习了一个对数正则化常数(他们学习的其实是\(-c_x\))，并且把它存储在一个以context为索引的哈希表中。带来的缺点是要引入大量的存储。但是论文中提到：他们发现固定正则化常数为1也就是令\(c_x=0\)也能取到相当不错的效果。他们解释为模型拥有了非常多自由的参数，能够大概满足自归一化条件，也就是满足假设3。</p>

<h3 id="toc_10">二分类目标和排序目标</h3>

<p>两种NCE方法大体上相同的，不同点在于：二分类目标是为了训练一个区分正负样本的二分类问题，也就是我们上文中提到的NCE方法；而排序目标是为了使得正确样本的排序能够在噪声样本之上。排序目标的NCE比二分类目标的NCE统计性质更好，它在假设2下是一致的，但是二分类目标在假设3下才是一致的。</p>

<p>定义：\[\bar{s}(x,y;\alpha) = s(x,y;\alpha) - \ln p_n(y)\]</p>

<p>排序目标：\[L^n_R(\alpha) = \frac{1}{n}\sum_{i=1}^{n}\ln\frac{\exp(\bar{s}(x_i,y_i;\alpha))}{\exp(\bar{s}(x_i,y_i;\alpha)) + \sum\limits_{j=1}^{K}\exp(\bar{s}(x_i,y_i^j;\alpha))}\]</p>

<p>二分类目标：\[L^n_B(\alpha,\gamma) = \frac{1}{(K+1)n}\sum\limits_{i=1}^{n}\{\ln \sigma(\bar{s}(x_i,y_i;\alpha) - \gamma - \ln K) + \sum\limits_{j=1}^K\ln[1-\sigma(\bar{s}(x_i,y_i^j;\alpha) - \gamma - \ln K)]\}\]</p>

<p>我们令\(L^\infty_R(\alpha)=\mathbb{E}[L^n_R(\alpha)]\), \(L^\infty_B(\alpha)=\mathbb{E}[L^n_B(\alpha)]\)</p>

<h3 id="toc_11">分析</h3>

<h4 id="toc_12">二分类目标</h4>

<p>根据定理2我们知道，在满足假设3的情况下二分类目标是一致的，并且最优化二分类目标的参数恰好满足：\[\ln p_d(y|x) = s(x,y,\alpha^*) - \gamma^*\] 但是只满足假设2的情况下估计有可能是不一致的，我们在接下来会给出一个具体的例子。</p>

<h4 id="toc_13">排序目标</h4>

<p>假设4: 对于任何参数\(\alpha\)，如果存在函数\(c(x)\)使得\(s(x,y;\alpha) - s(x,y;\alpha^*) = c(x),\forall x,y\)，那么就有\(\alpha = \alpha^*\)。</p>

<p>我们可以证明在满足假设2的情况下，基于排序目标的NCE算法能够得到强的一致估计。进一步的，如果假设4成立，那么\[P\{\lim\limits_{n\rightarrow \infty}\hat{\alpha_R^n}=\alpha^*\} = 1\]</p>

<h4 id="toc_14">例子</h4>

<p>\(X = \{0, 1\}\)，并且\(P(X = 0) = P(X=1) = 0.5\)，\(Y = \{0,1\}\)，并且条件概率分布由参数为\(\alpha = (\alpha_1,\alpha_2)\)分数函数和(1)式定义。<br/>
并且：\[s(0,0;\alpha) = \ln\alpha_1\] \[s(0,1;\alpha)=s(1,0;\alpha) = s(1,1;\alpha) = \ln\alpha_2\]</p>

<p>也就是说：\[p_d(y=0|x=0;\alpha) = \frac{\alpha_1}{\alpha_1+\alpha_2}\]<br/>
\[p_d(y=1|x=0;\alpha) = \frac{\alpha_2}{\alpha_1+\alpha_2}\]<br/>
\[p_d(y=0|x=1;\alpha) = p_d(y=1|x=1;\alpha) = 0.5\]</p>

<p>这样一来我们可以用表格表示联合概率：</p>

<table>
<thead>
<tr>
<th></th>
<th>x=0</th>
<th>x=1</th>
</tr>
</thead>

<tbody>
<tr>
<td>y=0</td>
<td>\(\frac{\alpha_1}{2(\alpha_1 + \alpha_2)}\)</td>
<td>0.25</td>
</tr>
<tr>
<td>y=1</td>
<td>\(\frac{\alpha_2}{2(\alpha_1 + \alpha_2)}\)</td>
<td>0.25</td>
</tr>
</tbody>
</table>

<p>我们的真实概率参数为\(\alpha^* = (1,3)\)，我们取的噪声函数为\(p_n(0) = p_n(1)=0.5\)，根据大数定理，当我们取到的样本足够多的情况下，二分类目标将会逼近其期望情况。</p>

<p>\[\begin{split}<br/>
L_B^\infty(\alpha,\gamma) &amp;= \frac{1}{8}\ln \sigma(\ln\theta_1 - \gamma - 0.5) + \frac78\ln \sigma(\ln\theta_2 - \gamma - 0.5) \\<br/>
&amp;~~~+ \frac{K}{4}\ln[1-\sigma(\ln\theta_1-\gamma-0.5)] + \frac{3K}{4}\ln[1-\sigma(\ln\theta_2-\gamma-0.5)]\\<br/>
&amp;= \frac{1}{8}\ln \frac{2\theta_1}{2\theta_1 + K\exp \gamma} + \frac78\ln \frac{2\theta_2}{2\theta_1 + K\exp \gamma} + \frac{K}{4}\ln[\frac{K\gamma}{2\theta_1 + K\exp \gamma}] \\<br/>
&amp;~~~+ \frac{3K}{4}\ln[\frac{K\gamma}{2\theta_1 + K\exp \gamma}]\\<br/>
\end{split}\]</p>

<p>经过计算我们可以得到最小化目标函数的条件是：\[\theta_1=\frac14\exp\gamma,\theta_2=\frac{7}{12}\exp\gamma\]这时候估计的条件概率为：<br/>
\[P(y=0|x=0) = \frac14, P(y=1|x=0)=\frac{7}{12}\]</p>

<p>\[P(y=0|x=1) = \frac{7}{12}, P(y=1|x=1)=\frac{7}{12}\]<br/>
显然这时候没有做到自归一化，因此无法给出条件分布的一致估计。</p>

<p>但是对于排序目标，其期望情况为：</p>

<p>\[\begin{split}<br/>
L_R^\infty(\alpha,\gamma) &amp;= \frac{1}{16}[\ln \frac{\alpha_1}{\alpha_1+\alpha2} + \ln \frac{\alpha_1}{\alpha_1+\alpha2}] \\<br/>
&amp;~~~\frac{3}{16}[\ln \frac{\alpha_2}{\alpha_2+\alpha1} + \ln \frac{\alpha_2}{\alpha_2+\alpha2}] \\<br/>
&amp;~~~\frac{1}{8}[\ln \frac{\alpha_2}{\alpha_2+\alpha2} + \ln \frac{\alpha_1}{\alpha_1+\alpha2}]<br/>
\\&amp;~~~\frac{1}{8}[\ln \frac{\alpha_2}{\alpha_2+\alpha2} + \ln \frac{\alpha_2}{\alpha_2+\alpha2}] \\<br/>
&amp;=\frac{1}{16}\ln\frac{\alpha_1}{\alpha_1+\alpha_2} + \frac{3}{16}\ln\frac{\alpha_2}{\alpha_1+\alpha_2} + \mathrm{Const}\\<br/>
\end{split}\]</p>

<p>最小化的条件恰好为：\(\alpha_2 = 3\alpha_1\)</p>

<h3 id="toc_15">实验观察</h3>

<ol>
<li>NCE估计对一切固定的K都是一致的，对于固定的数据集大小，NCE估计随着K的增大而估计的越准确，当K足够大后能够取得和MLE能够相比拟的效果。</li>
<li>数据集越大，NCE估计的参数对K越不敏感，当数据集足够大后，比较小的K也能够得到比较好的效果</li>
<li>当满足假设3的时候，二分类的NCE效果要好于MLE和排序的NCE，这是因为二分类的NCE模型能够自归一化。因此可以在排序NCE和MLE的目标函数后面加上正则项来促进模型的自归一化：\(\frac{\eta}{n}\sum\limits_{i=1}^n[\ln(\frac1m \sum\limits_{j=1}^m\exp(\bar s(x_i, y_i^j;\alpha)))]^2\)。这样做之后，排序NCE能够取得比二分类NCE更好的效果。</li>
</ol>

<h2 id="toc_16">NCE和Negative Sampling</h2>

<p>如果我们令\(\bar s(x,y;\alpha) = v_y^T \cdot v_x + \ln K\)，或者说等价的话\(s(x,y;\alpha) = v_y^T \cdot v_x + \ln p_n(y) + \ln K\)，就可以得到Negative Sampling。</p>

<p>\[L^n_B(\alpha,\gamma) = \frac{1}{(K+1)n}\sum\limits_{i=1}^{n}\{\ln \frac{\exp(v_y^T\cdot v_x)}{\exp(v_y^T\cdot v_x)+1} + \sum\limits_{j=1}^K\ln \frac{1}{\exp(v_y^T\cdot v_x)+1}\}\]</p>

<p>注意到：此时自归一化的条件是：\(\int \exp(v_y^T \cdot v_x)p_n(y)\mathrm{d}y = \frac{1}{K}\)，而不是\(\int \exp(v_y^T \cdot v_x)\mathrm{d}y = 1\)</p>

<h2 id="toc_17">参考文献：</h2>

<ol>
<li>Michael Gutmann and Aapo Hyvärinen. 2010. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proc. AISTATS.</li>
<li>Andriy Mnih and Yee W Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1751–1758.</li>
<li>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In NIPS.</li>
<li>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their composition- ality. In Proceedings of the 26th International Con- ference on Neural Information Processing Systems - Volume 2, NIPS’13, pages 3111–3119, USA. Curran Associates Inc.</li>
<li>Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency. arXiv preprint arXiv:1809.01812, 2018.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Word2Vec入门]]></title>
    <link href="https://sillybun.github.io/15540423884690.html"/>
    <updated>2019-03-31T22:26:28+08:00</updated>
    <id>https://sillybun.github.io/15540423884690.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Background</h2>

<p>@todo</p>

<h2 id="toc_1">Model Introduction</h2>

<p>在Word2Vec中，Mikolov提出了两个模型 — Continuous Bag-of-Words（CBOW）模型和Skip-gram（SG）模型。两个模型都有三层：输入层，投影层和输出层。对于CBOW模型，输入上下文预测单词；对于SG模型，通过单词预测上下文。目前来看，一般认为SG的效果好于CBOW模型。</p>

<span id="more"></span><!-- more -->

<h3 id="toc_2">CBOW Model</h3>

<p>CBOW包含三层：</p>

<ol>
<li>输入层，包含了context(w)中的2n个单词：\(e(w_{-n}), e(w_{-n+1}), \dots, e(w_n)\)</li>
<li>投影层：投影层是输入层的求和，即：\(x_w = \sum\limits_{i=-n}^{n}e(w_i)\)</li>
<li>输出层：有两种方法：一种是多层的softmax，一种是negative sampling<br/>
优化的函数是：<br/>
\[<br/>
L = \sum\limits_{w\in C}\ln \mathrm{P}(w|\mathrm{context}(w))<br/>
\]</li>
</ol>

<h3 id="toc_3">Hierarchical Softmax</h3>

<p>由于如果对词典中的每个单词使用softmax的计算量非常的大，Word2Vec使用了多层的Softmax方法。我们根据词频建立的<a href="15540295441807.html">Huffman树</a>，这棵树包含了\(|D|\)个叶子和\(|D|-1\)个内点，每个叶子节点对应着词典中的一个单词，出现频率比较高的单词深度会比较的浅，因此到达它的路径会更短。由于到叶子的平均深度为\(\log_2|D|\)，所以对于每一个\((w, \mathrm{context}(w))\)，需要训练的向量的个数的数学期望为：\(\log_2|D|\)。<br/>
到每个叶子节点的路径是唯一的，我们做如下的约定：令p为从根到叶子节点w的路径，令\(n(w, j)\)是p上的第j个节点，\(L(w)\)是p的长度，则\(n(w,1)=\mathrm{root}\)，\(n(w, L(w))=w\)。对于任何的内点n，我们令\(ch(n)\)为n的任何一个选定的子节点，不失一般性的我们可以令它为n的左子节点。令\([\![x]\!]\)为1，如果x为真，否则为-1，那么我们可以根据如下公式定义\(p(w_O|w_I)\)：<br/>
\[<br/>
p(w|w_I) = \prod\limits_{j=1}^{L(w)-1}\sigma([\![n(w, j+1)=ch(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T v_{w_I})<br/>
\]<br/>
我们下面证明：<br/>
引理1：\(\sum\limits_{w\in C} p(w|w_I) = 1\)<br/>
证明：<br/>
令T是C构成的Huffman二叉树，根据Huffman树的性质是我们可以令x和y为T中深度最深的两个兄弟，我们在T中去掉x和y，并且把它们的父亲节点命名为z，得到的新的词表称为\(C’\)(\(C’ = C - \{x,y\} + \{z\}\))，对应的完全二叉树命名为\(T’\)。我们下面证明：<br/>
\[<br/>
\sum\limits_{w\in C} p(w|w_I) = \sum\limits_{w\in C’} p’(w|w_I)\<br/>
\]<br/>
事实上：<br/>
\[<br/>
\begin{split}<br/>
\sum\limits_{w\in C} p(w|w_I) - \sum\limits_{w\in C’} p’(w|w_I) &amp;= p(x|w_I) +p(y|w_I) - p(z|w_I)\\<br/>
&amp;=\prod\limits_{j=1}^{L(x)-1}\sigma([\![n(x, j+1)=ch(n(x,j))]\!]\cdot v_{n(x,j)}&#39;^T v_{w_I})\\<br/>
&amp;~~~~+\prod\limits_{j=1}^{L(y)-1}\sigma([\![n(y, j+1)=ch(n(y,j))]\!]\cdot v_{n(y,j)}&#39;^T v_{w_I})\\<br/>
&amp;~~~~-\prod\limits_{j=1}^{L(z)-1}\sigma([\![n(z, j+1)=ch(n(z,j))]\!]\cdot v_{n(z,j)}&#39;^T v_{w_I})\\<br/>
\end{split}<br/>
\]<br/>
显然有这些关系：<br/>
\[<br/>
\begin{align*}<br/>
L(x)=L(y)=L(z)+1\\<br/>
n(x,j) = n(y,j)=n(z,j), \forall 1\leq j\leq L(z)\\<br/>
\end{align*}<br/>
\]<br/>
因此<br/>
\[<br/>
\begin{split}<br/>
&amp;\sum\limits_{w\in C} p(w|w_I) - \sum\limits_{w\in C’} p’(w|w_I)\\<br/>
&amp;= \prod\limits_{j = 1}^{L(z)-1}\sigma([\![n(z,j+1)=ch(n(z,j))]\!]\cdot v_{n(z,j)}&#39;^Tv_{w_I})\\<br/>
&amp;(\sigma([\![x=ch(n(z,L(z))) ]\!] v_{n(z,L(z))}&#39;^T v_{w_I}) + \sigma([\![y=ch(n(z, L(z)))]\!]v_{n(z,L(z))}&#39;^T v_{w_I}) -1)\\<br/>
&amp;=0<br/>
\end{split}<br/>
\]<br/>
最后一个等式是因为：\(\sigma(x) + \sigma(-x) = 1, \forall x\in\mathbf{R}\)。</p>

<p>这样我们通过不断合并最深的两个兄弟节点，就可以最终把Huffman树合并成只有一个根的树，这样我们就证明了引理1的正确性。</p>

<hr/>

<p>根据引理1，我们通过（1）式定义的是一个概率测度。我们可以通过最最大似然法来进行参数估计。</p>

<h3 id="toc_4">梯度计算</h3>

<p>对于CBOW，投影层之后紧跟着的就是hierarchical softmax层，所以：</p>

<p>\[v_{w_I} = x_w = \sum\limits_{i=-n}^{n}e(w_i)\]</p>

<p>\[\mathrm{P}(w|\mathrm{context}(w)) = \prod\limits_{j=1}^{L(w)-1}\sigma([\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)\]</p>

<p>我们优化的目标为对数似然函数：</p>

<p>\[<br/>
\begin{split}<br/>
\ell &amp;= \sum\limits_{w\in C}\ln \mathrm{P}(w|\mathrm{context}(w))\\<br/>
&amp;= \sum\limits_{w\in C}\sum\limits_{j=1}^{L(w)-1}\ln{\sigma([\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)}<br/>
\end{split}<br/>
\]</p>

<p>我们令：\[L(w,j) = \ln{\sigma([\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)}\]</p>

<p>在huffman编码中，如果我们令\([\![n(w,j+1)=\mathrm{ch}(n(w,j))]\!]\)为1时编码为0，为-1时编码为1，并且令这个数为\(d(w,j)\)，表示w的huffman编码的第j个数。</p>

<p>\[<br/>
\begin{split}<br/>
\frac{\partial L(w,j)}{\partial v_{n(w,j)}} &amp;= \sigma(-[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]x_w\\<br/>
&amp;= [1 - d(w,j) - \sigma(v_{n(w,j)}&#39;^T x_w)]x_w<br/>
\end{split}<br/>
\]</p>

<p>\[<br/>
\begin{split}<br/>
\frac{\partial L(w,j)}{\partial x_w} &amp;= \sigma(-[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]v_{n(w,j)}&#39;\\<br/>
&amp;= [1 - d(w,j) - \sigma(v_{n(w,j)}&#39;^T x_w)]v_{n(w,j)}<br/>
\end{split}<br/>
\]</p>

<h2 id="toc_5">Negative Sampling</h2>

<p>在我的文章<a href="15541232015450.html">Noise Contrastive Estimation and Negative Sampling</a>中，详细介绍了NCE和Negative Sampling。Mikolov指出对于小的训练数据\(K\)取5-20，而对于比较大的训练数据\(K\)取2-5就可以了。Negative Sampling的简单之处在于它认为模型可以包含噪声分布，因此在计算中不需要噪声分布。</p>

<p>我们优化的目标为对数似然函数：</p>

<p>\[<br/>
\begin{split}<br/>
\ell &amp;= \sum\limits_{w\in C}\ln \mathrm{P}(w|\mathrm{context}(w))\\<br/>
&amp;= \sum\limits_{y\in C}[\ln\frac{\exp(v_y^Tx_y)}{\exp(v_y^Tx_y) + 1}+\sum\limits_{j=1,y_j\sim p_n(y)}^{K}\ln\frac{1}{\exp(v_{y_j}^Tx_y) + 1}]<br/>
\end{split}<br/>
\]</p>

<p>如果模型足够强大的话，得到的结果应该满足自归一化条件，也就是说：\[\sum\limits_{y\in \mathbf{V}}\exp(v_y^T x_y)p_n(y)K=1\]</p>

<p>对于NCE和Negative Sampling来说噪声分布都是一个自由的参数，Mikolov指出取\(U(w)^{3/4}/Z\)是最好的选择，也就是说：\[p_n(y)=\frac{f(y)^{3/4}}{\sum\limits_{t\in \mathbf{Y}}f(t)^{3/4}}\]</p>

<h2 id="toc_6">Skip-Gram模型</h2>

<p>SG的目标在于通过预测一个单词的上下文来得到词表示，也就是说，我们的目标是最大化平均对数概率：\[\frac1T\sum_{t=1}^T\sum_{w\in \mathrm{context}}\ln p(w|w_t)\]</p>

<p>最基础的SG模型中使用softmax函数来定义概率：\[p(y|x) = \frac{\exp(v_y^Tv_x)}{\sum\limits_{t\in\mathbf{Y}}\exp(v_t^Tv_x)}\]</p>

<h3 id="toc_7">常见词的降采样</h3>

<p>在文本中，一些非常常见的词会经常出现，但是这些单词能够提供的信息比稀有的单词要少得多，为了利用这个特点，SG使用了降采样的方法，忽略根据单词出现的频率忽略常见单词，忽略的概率为：</p>

<p>\[<br/>
P(w) = 1 - \sqrt{\frac{t}{f(w)}}<br/>
\]</p>

<p>其中\(f(w)\)是w这个单词出现的频率，t是阈值，通常设为\(10^-5\)，出现频率小于t的单词不会被降采样，而出现频率大于t的单词会被忽略。</p>

<h2 id="toc_8">参考文献</h2>

<ol>
<li>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their composition- ality. In Proceedings of the 26th International Con- ference on Neural Information Processing Systems - Volume 2, NIPS’13, pages 3111–3119, USA. Curran Associates Inc.</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Huffman树]]></title>
    <link href="https://sillybun.github.io/15540295441807.html"/>
    <updated>2019-03-31T18:52:24+08:00</updated>
    <id>https://sillybun.github.io/15540295441807.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">问题</h2>

<p>Huffman树是为了解决这样的问题：给一段文字，能不能给出一个编码方式使得这段文字长度最短。</p>

<h2 id="toc_1">思考</h2>

<p>比如对于<code>aabc</code>，如果我们a编码成00，b编码成01，c编码成10，那么这句话表示为：<code>00000110</code>，一共有8个比特。但是如果a编码成0，b编码成10，c编码成01，那么这句话表示为<code>001011</code>，一共有6个比特。<br/>
看到了么？不同的编码方式下同样的内容对应的长度是不相同的，如何得到一段文字的最佳编码方式呢？Huffman提供了一种算法，他提供了一个贪心的解决了编码问题，并且证明了这种贪心算法得到的答案是最优解。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_2">定义</h2>

<p>我们首先作出以下定义：<br/>
<strong>前缀编码</strong>：如果一个编码方式中没有一个编码是另一个编码的前缀，我们称这种编码方式为前缀编码。<br/>
我们可以看到，如果一种编码方式不是前缀编码，可能会出现二意性的问题，在使用上有很大的不方便。事实上，可以证明，最佳编码方式一定有一种是前缀编码，所以我们把我们的讨论范围限制在前缀编码上。<br/>
我们可以使用一颗二叉树来表示我们的编码方式，每个叶子结点代表一个字符，它的编码由根结点到它的唯一路径表示，如果是左子结点对应着编码0，如果是右子结点对应着编码0。这样的话对于深度为depth（这里认为根结点对应的深度为0）的字符它的编码长度为depth。在这棵二叉树上越浅的位置的字符拥有越少的编码长度。这样来看，我们应该把出现频率高的字符放置在二叉树比较浅的位置，出现频率低的字符放在二叉树比较深的位置。<br/>
显然，我们可以进一步的把我们的讨论范围局限在一颗完全二叉树上面来，这是因为，如果有一个结点只有1个子节点，我们完全可以把这个父亲结点取消掉，直接把子结点连接到父节点的父亲上。子树上的每个叶子结点对应的编码长度都会减少1。<br/>
这样的话，设一种编码方式对应的二叉树为T，对于字符集中的每一个字符c，我们令\(c.freq\)表示c出现的次数，\(d_T(c)\)表示c在树中的深度，这样需要编码这个文件的比特数为：<br/>
\[<br/>
B(T) = \sum\limits_{c\in C}c.freq \cdot d_T(c)<br/>
\]</p>

<h2 id="toc_3">Huffman树的建立</h2>

<p>我们作出以下约定：</p>

<ol>
<li>C表示字符集，里面含有n个字符，其中的每个字符c有一个属性\(c.freq\)</li>
<li>Q表示一个最小堆（优先级队列）<br/>
建立Huffman树的伪代码如下所示：</li>
</ol>

<pre class="line-numbers"><code class="language-python">Huffman(C)
N = |C|
Q = C
for i = 1 to n-1
    allocate a new node z
    z.left = x = Extract-Min(Q)
    z.right = y = Extract-Min(Q)
    z.freq = x.freq + y.freq
    Insert(Q, z)
return Extract-Min(Q)
</code></pre>

<h3 id="toc_4">时间复杂度分析</h3>

<p>建立最小堆的时间复杂度为\(O(n)\)，一共有n-1次循环，每一次循环的时间复杂度为\(O\log n\)，所以Huffman算法的时间复杂度为\(O(n\log n)\)</p>

<h2 id="toc_5">算法正确性的证明</h2>

<p>我们需要证明两个引理。</p>

<hr/>

<p>引理一：对于C中出现次数最低的两个字符x,y，一定存在着一种编码方式，使得x和y的编码长度一样，除了最后一位外其他的位置都相同。<br/>
证明：<br/>
我们只要证明在对应的二叉树上，x和y是兄弟结点就可以了。<br/>
对于一棵最优编码对应的二叉树T，设a和b是它深度最深的叶子兄弟（它们是叶子结点并且它们是兄弟）。假如a和b不是x和y，不妨假设x不是a也不是b。<br/>
我们对换x和a的位置，y和b的位置，得到了一棵新的二叉树\(T’\)<br/>
\[<br/>
\begin{split}B(T) - B(T’) &amp;= x.freq\cdot d_T(x) + a.freq\cdot d_T(a)-x.freq\cdot d_T(a) - a.freq\cdot d_T(x) + \dots \\<br/>
&amp;= (a.freq - x.freq) (d_T(a)-d_T(x)) +  (b.freq - y.freq) (d_T(b)-d_T(y))\\<br/>
&amp;\geq 0<br/>
\end{split}<br/>
\]<br/>
但是由于T是最佳二叉树，所以\(B(T) = B(T’)\)，所以\(T’\)也是完全二叉树，并且在\(T’\)中，x和y是最深的兄弟结点。<br/>
引理二：对于C中出现的次数最低的两个字符x,y，我们在C中去掉x和y，加入一个新的字符z，并且令\(z.freq=x.freq + y.freq\)，建立一个新的字符集\(C’\)。那么对于\(C’\)的最佳二叉树\(T’\)，我们把z对应的叶子结点去掉，放置一个内部结点，并且以x和y作为两个子节点对应的完全二叉树为C对应的一棵最佳二叉树。<br/>
证明：<br/>
我们首先说明\(B(T)\)和\(B(T’)\)之间的关系：\(d_{T’}(z) = d_T(x) + 1\)。<br/>
\[\begin{split}<br/>
B(T) - B(T’) &amp;= x.freq \cdot d_T(x) +y.freq \cdot d_T(y) - z.freq \cdot d_{T’}(z)\\<br/>
&amp;=x.freq \cdot d_T(x) +y.freq \cdot d_T(y) - (x.freq + y.freq) \cdot (d_{T}(x) - 1)\\<br/>
&amp;=x.freq + y.freq<br/>
\end{split}<br/>
\]<br/>
假设T不是C的一棵最佳二叉树，设\(T’’\)是C的一棵最佳二叉树，且根据引理1不妨假设x和y位于最深处并且x和y是兄弟。那么我们把x和y去掉并且把它们的父亲结点命名为z并且令\(z = x.freq + y.freq\)就构成了\(C’\)的一棵二叉树\(T’’’\)</p>

<p>\[\begin{split}<br/>
B(T’’’) &amp;= B(T’’) - x.freq - y.freq\\<br/>
&amp;&lt; B(T) - x.freq - y.freq\\<br/>
&amp;= B(T’) <br/>
\end{split}\]</p>

<p>这与\(T’\)是\(C’\)的最佳二叉树矛盾</p>

<hr/>

<p>根据引理一和引理二我们可以得到Huffman算法的正确性。~~~~</p>

]]></content>
  </entry>
  
</feed>
