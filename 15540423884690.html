<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  Word2Vec入门 - 张翼腾的博客
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="张翼腾的博客" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:sillybun.github.io ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="https://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; 张翼腾的博客</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        

    <li><label>Categories</label></li>

         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
  $(function(){
    $('#menu_item_index').addClass('is_active');
  });
</script>
<div class="row">
  <div class="large-8 medium-8 columns">
      <div class="markdown-body article-wrap">
       <div class="article">
          
          <h1>Word2Vec入门</h1>
     
        <div class="read-more clearfix">
          <span class="date">2019/3/31</span>

          
           
         
          <span class="comments">
            

            
          </span>

        </div>
      </div><!-- article -->

      <div class="article-content">
      <h2 id="toc_0">Background</h2>

<p>@todo</p>

<h2 id="toc_1">Model Introduction</h2>

<p>在Word2Vec中，Mikolov提出了两个模型 — Continuous Bag-of-Words（CBOW）模型和Skip-gram（SG）模型。两个模型都有三层：输入层，投影层和输出层。对于CBOW模型，输入上下文预测单词；对于SG模型，通过单词预测上下文。目前来看，一般认为SG的效果好于CBOW模型。</p>

<span id="more"></span><!-- more -->

<h3 id="toc_2">CBOW Model</h3>

<p>CBOW包含三层：</p>

<ol>
<li>输入层，包含了context(w)中的2n个单词：\(e(w_{-n}), e(w_{-n+1}), \dots, e(w_n)\)</li>
<li>投影层：投影层是输入层的求和，即：\(x_w = \sum\limits_{i=-n}^{n}e(w_i)\)</li>
<li>输出层：有两种方法：一种是多层的softmax，一种是negative sampling<br/>
优化的函数是：<br/>
\[<br/>
L = \sum\limits_{w\in C}\ln \mathrm{P}(w|\mathrm{context}(w))<br/>
\]</li>
</ol>

<h3 id="toc_3">Hierarchical Softmax</h3>

<p>由于如果对词典中的每个单词使用softmax的计算量非常的大，Word2Vec使用了多层的Softmax方法。我们根据词频建立的<a href="15540295441807.html">Huffman树</a>，这棵树包含了\(|D|\)个叶子和\(|D|-1\)个内点，每个叶子节点对应着词典中的一个单词，出现频率比较高的单词深度会比较的浅，因此到达它的路径会更短。由于到叶子的平均深度为\(\log_2|D|\)，所以对于每一个\((w, \mathrm{context}(w))\)，需要训练的向量的个数的数学期望为：\(\log_2|D|\)。<br/>
到每个叶子节点的路径是唯一的，我们做如下的约定：令p为从根到叶子节点w的路径，令\(n(w, j)\)是p上的第j个节点，\(L(w)\)是p的长度，则\(n(w,1)=\mathrm{root}\)，\(n(w, L(w))=w\)。对于任何的内点n，我们令\(ch(n)\)为n的任何一个选定的子节点，不失一般性的我们可以令它为n的左子节点。令\([\![x]\!]\)为1，如果x为真，否则为-1，那么我们可以根据如下公式定义\(p(w_O|w_I)\)：<br/>
\[<br/>
p(w|w_I) = \prod\limits_{j=1}^{L(w)-1}\sigma([\![n(w, j+1)=ch(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T v_{w_I})<br/>
\]<br/>
我们下面证明：<br/>
引理1：\(\sum\limits_{w\in C} p(w|w_I) = 1\)<br/>
证明：<br/>
令T是C构成的Huffman二叉树，根据Huffman树的性质是我们可以令x和y为T中深度最深的两个兄弟，我们在T中去掉x和y，并且把它们的父亲节点命名为z，得到的新的词表称为\(C’\)(\(C’ = C - \{x,y\} + \{z\}\))，对应的完全二叉树命名为\(T’\)。我们下面证明：<br/>
\[<br/>
\sum\limits_{w\in C} p(w|w_I) = \sum\limits_{w\in C’} p’(w|w_I)\<br/>
\]<br/>
事实上：<br/>
\[<br/>
\begin{split}<br/>
\sum\limits_{w\in C} p(w|w_I) - \sum\limits_{w\in C’} p’(w|w_I) &amp;= p(x|w_I) +p(y|w_I) - p(z|w_I)\\<br/>
&amp;=\prod\limits_{j=1}^{L(x)-1}\sigma([\![n(x, j+1)=ch(n(x,j))]\!]\cdot v_{n(x,j)}&#39;^T v_{w_I})\\<br/>
&amp;~~~~+\prod\limits_{j=1}^{L(y)-1}\sigma([\![n(y, j+1)=ch(n(y,j))]\!]\cdot v_{n(y,j)}&#39;^T v_{w_I})\\<br/>
&amp;~~~~-\prod\limits_{j=1}^{L(z)-1}\sigma([\![n(z, j+1)=ch(n(z,j))]\!]\cdot v_{n(z,j)}&#39;^T v_{w_I})\\<br/>
\end{split}<br/>
\]<br/>
显然有这些关系：<br/>
\[<br/>
\begin{align*}<br/>
L(x)=L(y)=L(z)+1\\<br/>
n(x,j) = n(y,j)=n(z,j), \forall 1\leq j\leq L(z)\\<br/>
\end{align*}<br/>
\]<br/>
因此<br/>
\[<br/>
\begin{split}<br/>
&amp;\sum\limits_{w\in C} p(w|w_I) - \sum\limits_{w\in C’} p’(w|w_I)\\<br/>
&amp;= \prod\limits_{j = 1}^{L(z)-1}\sigma([\![n(z,j+1)=ch(n(z,j))]\!]\cdot v_{n(z,j)}&#39;^Tv_{w_I})\\<br/>
&amp;(\sigma([\![x=ch(n(z,L(z))) ]\!] v_{n(z,L(z))}&#39;^T v_{w_I}) + \sigma([\![y=ch(n(z, L(z)))]\!]v_{n(z,L(z))}&#39;^T v_{w_I}) -1)\\<br/>
&amp;=0<br/>
\end{split}<br/>
\]<br/>
最后一个等式是因为：\(\sigma(x) + \sigma(-x) = 1, \forall x\in\mathbf{R}\)。</p>

<p>这样我们通过不断合并最深的两个兄弟节点，就可以最终把Huffman树合并成只有一个根的树，这样我们就证明了引理1的正确性。</p>

<hr/>

<p>根据引理1，我们通过（1）式定义的是一个概率测度。我们可以通过最最大似然法来进行参数估计。</p>

<h3 id="toc_4">梯度计算</h3>

<p>对于CBOW，投影层之后紧跟着的就是hierarchical softmax层，所以：</p>

<p>\[v_{w_I} = x_w = \sum\limits_{i=-n}^{n}e(w_i)\]</p>

<p>\[\mathrm{P}(w|\mathrm{context}(w)) = \prod\limits_{j=1}^{L(w)-1}\sigma([\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)\]</p>

<p>我们优化的目标为对数似然函数：</p>

<p>\[<br/>
\begin{split}<br/>
\ell &amp;= \sum\limits_{w\in C}\ln \mathrm{P}(w|\mathrm{context}(w))\\<br/>
&amp;= \sum\limits_{w\in C}\sum\limits_{j=1}^{L(w)-1}\ln{\sigma([\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)}<br/>
\end{split}<br/>
\]</p>

<p>我们令：\[L(w,j) = \ln{\sigma([\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)}\]</p>

<p>在huffman编码中，如果我们令\([\![n(w,j+1)=\mathrm{ch}(n(w,j))]\!]\)为1时编码为0，为-1时编码为1，并且令这个数为\(d(w,j)\)，表示w的huffman编码的第j个数。</p>

<p>\[<br/>
\begin{split}<br/>
\frac{\partial L(w,j)}{\partial v_{n(w,j)}} &amp;= \sigma(-[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]x_w\\<br/>
&amp;= [1 - d(w,j) - \sigma(v_{n(w,j)}&#39;^T x_w)]x_w<br/>
\end{split}<br/>
\]</p>

<p>\[<br/>
\begin{split}<br/>
\frac{\partial L(w,j)}{\partial x_w} &amp;= \sigma(-[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]\cdot v_{n(w,j)}&#39;^T x_w)[\![n(w, j+1)=\mathrm{ch}(n(w,j))]\!]v_{n(w,j)}&#39;\\<br/>
&amp;= [1 - d(w,j) - \sigma(v_{n(w,j)}&#39;^T x_w)]v_{n(w,j)}<br/>
\end{split}<br/>
\]</p>

<h2 id="toc_5">Negative Sampling</h2>

<p>在我的文章<a href="15541232015450.html">Noise Contrastive Estimation and Negative Sampling</a>中，详细介绍了NCE和Negative Sampling。Mikolov指出对于小的训练数据\(K\)取5-20，而对于比较大的训练数据\(K\)取2-5就可以了。Negative Sampling的简单之处在于它认为模型可以包含噪声分布，因此在计算中不需要噪声分布。</p>

<p>我们优化的目标为对数似然函数：</p>

<p>\[<br/>
\begin{split}<br/>
\ell &amp;= \sum\limits_{w\in C}\ln \mathrm{P}(w|\mathrm{context}(w))\\<br/>
&amp;= \sum\limits_{y\in C}[\ln\frac{\exp(v_y^Tx_y)}{\exp(v_y^Tx_y) + 1}+\sum\limits_{j=1,y_j\sim p_n(y)}^{K}\ln\frac{1}{\exp(v_{y_j}^Tx_y) + 1}]<br/>
\end{split}<br/>
\]</p>

<p>如果模型足够强大的话，得到的结果应该满足自归一化条件，也就是说：\[\sum\limits_{y\in \mathbf{V}}\exp(v_y^T x_y)p_n(y)K=1\]</p>

<p>对于NCE和Negative Sampling来说噪声分布都是一个自由的参数，Mikolov指出取\(U(w)^{3/4}/Z\)是最好的选择，也就是说：\[p_n(y)=\frac{f(y)^{3/4}}{\sum\limits_{t\in \mathbf{Y}}f(t)^{3/4}}\]</p>

<h2 id="toc_6">Skip-Gram模型</h2>

<p>SG的目标在于通过预测一个单词的上下文来得到词表示，也就是说，我们的目标是最大化平均对数概率：\[\frac1T\sum_{t=1}^T\sum_{w\in \mathrm{context}}\ln p(w|w_t)\]</p>

<p>最基础的SG模型中使用softmax函数来定义概率：\[p(y|x) = \frac{\exp(v_y^Tv_x)}{\sum\limits_{t\in\mathbf{Y}}\exp(v_t^Tv_x)}\]</p>

<h3 id="toc_7">常见词的降采样</h3>

<p>在文本中，一些非常常见的词会经常出现，但是这些单词能够提供的信息比稀有的单词要少得多，为了利用这个特点，SG使用了降采样的方法，忽略根据单词出现的频率忽略常见单词，忽略的概率为：</p>

<p>\[<br/>
P(w) = 1 - \sqrt{\frac{t}{f(w)}}<br/>
\]</p>

<p>其中\(f(w)\)是w这个单词出现的频率，t是阈值，通常设为\(10^-5\)，出现频率小于t的单词不会被降采样，而出现频率大于t的单词会被忽略。</p>

<h2 id="toc_8">参考文献</h2>

<ol>
<li>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their composition- ality. In Proceedings of the 26th International Con- ference on Neural Information Processing Systems - Volume 2, NIPS’13, pages 3111–3119, USA. Curran Associates Inc.</li>
</ol>


    

      </div>

      <div class="row">
        <div class="large-6 columns">
        <p class="text-left" style="padding:15px 0px;">
      
          <a href="15541232015450.html" 
          title="Previous Post: Noise Contrastive Estimation and Negative Sampling">&laquo; Noise Contrastive Estimation and Negative Sampling</a>
      
        </p>
        </div>
        <div class="large-6 columns">
      <p class="text-right" style="padding:15px 0px;">
      
          <a  href="15540295441807.html" 
          title="Next Post: Huffman树">Huffman树 &raquo;</a>
      
      </p>
        </div>
      </div>
      <div class="comments-wrap">
        <div class="share-comments">
          

          

          
        </div>
      </div>
    </div><!-- article-wrap -->
  </div><!-- large 8 -->




 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <h1>张翼腾的博客</h1>
                <div class="site-des">主要关注点在于自然语言处理领域，作者在学习的过程中分享自己学到的新的内</div>
                <div class="social">










<a target="_blank" class="email" href="mailto:15307130114@fudan.edu.cn" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15547144642483.html">矩阵微分计算</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15546453666266.html">向量微分计算</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15543921008119.html">信息论</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15541232015450.html">Noise Contrastive Estimation and Negative Sampling</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15540423884690.html">Word2Vec入门</a>
			      </li>
		     
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-137432635-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-137432635-1');
</script>


  </body>
</html>
